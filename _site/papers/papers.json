[
  {
    "path": "papers/2019-05-15-bepu-ce-mc-n-p-transport-3loop-bioshield/",
    "title": "Best-Estimate Plus Uncertainty Analysis for Continuous Energy Monte Carlo Coupled Neutron-Gamma Transport Simulations Through a 3-Loop Westinghouse PWR Concrete Biological Shield Wall",
    "description": "",
    "author": [
      {
        "name": "Amrit Patel",
        "url": {}
      }
    ],
    "date": "2019-05-15",
    "categories": [],
    "contents": "\n\nContents\nRegulatory Background\nObjectives\nRegulatory\nResearch\n\nModel Setup\nInitial BE Work with SCALE-MAVRIC\nUQ Methods Studied\nLatin Hypercube Sampling\nPolynomial Chaos Expansion\nProbability Bounds Analyis\n\nUQ Exploration Using Max Bioshield 0-Dimension Responses\nInputs Studied\nLHS Reference Results\nPCE Sobol Indeces and Sensitivity Coefficients\nPBA Convergence\n\nUQ Exploration Using Max Bioshield 1-D Responses\nResponses with Uncertainties Through Bioshield\nUncertainties Through the Bioshield\nRadiation Damage Depth Based on the Maximum Uncertainty\n\nConclusions and Future Study\n\nRegulatory Background\nGuidance for subsequent license renewal applicants in NUREG-2192, “Standard Review Plan for Review of Subsequent License Renewal Applications for Nuclear Power Plants,” (or SRP-SLR), Section 3.5.2.2.2.6, “Reduction of Strength and Mechanical Properties of Concrete Due to Irradiation,” describes a method for determining whether the applicant has met the requirements of the NRC regulations in 10 CFR 54.21 by providing the acceptance criterion for the aging management of the reduction of strength and mechanical properties of concrete due to irradiation as it pertains to the reactor biological shield (or bioshield) wall (NRC 2017).\nRegarding the assessment of irradiation damage, the guidance states:\n\nReduction of strength, loss of mechanical properties, and cracking due to irradiation could occur in [pressurized water reactor (PWR)] and [boiling water reactor] Group 4 concrete structures that are exposed to high levels of neutron and gamma radiation. These structures include the reactor (primary/biological) shield wall, the sacrificial shield wall, and the reactor vessel support/pedestal structure. Data related to the effects and significance of neutron and gamma radiation on concrete mechanical and physical properties is limited, especially for conditions (dose, temperature, etc.) representative of light-water reactor (LWR) plants. However, based on literature review of existing research, radiation fluence limits of 1 × 1019 [neutrons per square centimeter (n/cm2)] neutron radiation and 1 × 108 [Gray (Gy)] (1 × 1010 rad) gamma dose are considered conservative radiation exposure levels beyond which concrete material properties may begin to degrade markedly.\n\n\nFurther evaluation is recommended of a plant-specific program to manage aging effects of irradiation if the estimated (calculated) fluence levels or irradiation dose received by any portion of the concrete from neutron (fluence cutoff [energy greater than 0.1 million-electron-volts [(E > 0.1 MeV)] or gamma radiation exceeds the respective threshold level during the subsequent period of extended operation or if plant-specific [operating experience] of concrete irradiation degradation exists that may impact intended functions. Higher fluence or dose levels may be allowed in the concrete if tests and/or calculations are provided to evaluate the reduction in strength and/or loss of mechanical properties of concrete from those fluence levels, at or above the operating temperature experienced by the concrete, and the effects are applied to the design calculations. Supporting calculations/analyses, test data, and other technical basis are provided to estimate and evaluate fluence levels and the plant-specific program. The acceptance criteria are described in BTP RLSB-1 (Appendix A.1 of this SRP-SLR).\n\nThe NRC has performed confirmatory studies to evaluate the maximum fluence/dose levels throughout a representative Westinghouse 3-loop PWR bioshield in support of a first-of-a-kind review of the Florida Power & Light Company’s Turkey Point Unit-3,4 subsequent license renewal application. The objective was to confirm the applicant’s best-estimate (BE) maximum fluence/dose estimates given that this type of fluence evaluation is not within the scope of existing regulatory guidance for calculational fluence methodologies, which is documented in NRC (2001). Specifically, the existing guidance is geared toward BE plus uncertainty high energy neutron fluence (i.e., energies greater that 1 mega electron volt - E > 1 MeV - evaluations) of reactor pressure vessels near the reactor core midplane. That is, the guidance does not consider the effect on calculation qualification due to:\nThe portion of the neutron spectrum between E > 0.1 Mev and E > 1 MeV,\nCoupled neutron and gamma transport, and\nAdditional sensitivity/uncertainty of bioshield material characteristics.1\nObjectives\nRegulatory\nThe regulatory objective of the study is to perform high fidelity simulations to understand:\nThe range of calculational uncertainties for the maximum neutron flux and gamma dose rates (i.e. “the responses”) throughout a representative bioshield.\nThe relative importance of bioshield material uncertainties on predicted “damage depth.”2\nResearch\nCoupled neutron-gamma transport calculations for characterization of bioshield material degradation have not been extensively studied. This is partly due to the fact that nuclear reactors in the U.S. were not envisioned to receive multiple license extensions, resulting in irradiation damage increasing in scope (i.e., number of components affected) over time. However, it is now being predicted that some bioshields (PWRs in particular) will now exceed acceptable damage thresholds at least for some fraction of these massive structures. Research is also ongoing to refine understanding of radiation damage thresholds (NRC (2013); Remec et al. (2018)). Therefore, the research objective is to glean insights into methods for extracting useful uncertainty information in support of the regulatory objective defined above, and further to determine the viability of various uncertainty quantification (UQ) methods to address:\nDetermining estimates of uncertainty from sampling a high dimension input space.\nRanking important contributors to response uncertainty.\nEstimating sensitivity coefficients of important contributors to the responses.\nFinally, an approach to characterizing response uncertainty in light of Regulatory Objective 2 listed above will be recommended.\nModel Setup\nContinuous energy Monte Carlo (CE-MC), using the MAVRIC sequence within the SCALE computer code package (Rearden and Jessee 2018), was selected for this study because it requires less modeling time and less approximation over the problem domain (i.e., space, energy, angle) relative to the current industry-standard practice of using deterministic methods such as discrete ordinates (or the method of characteristics),3 and solution acceleration techniques can now be utilized to get response statistics in a very reasonable amount of time on modest computational platforms (Wagner, Blakeman, and Peplow 2009).\nFor this work, all calculations were performed on an Amazon EC2 General Purpose compute M5 instance.4\nInitial BE Work with SCALE-MAVRIC\nInternal studies at the NRC over the past several years have aimed at benchmarking the SCALE/MAVRIC simulation tool to standard shielding benchmarks discussed in Regulatory Guide 1.190 (NRC 2001) – namely VENUS-3 experimental and H.B. Robinson, Unit 2 (HBR-2) operating reactor benchmarks. SCALE/MAVRIC results compared very well for both benchmarks. The main finding of these studies was that SCALE/MAVRIC is well-suited for efficient (i.e., single processor wall clock time on the order of hours) and high-fidelity (i.e., three-dimensional space and continuous energy physics) shielding studies to inform regulatory activities – confirmatory analyses and guidance development in particular.\nA variant of the HBR-2 benchmark was modeled in this study mostly consistent with the model described in Field, Remec, and Le Pape (2015).5\nUQ Methods Studied\nTo address the research objectives, multiple UQ methods were studied using DAKOTA (Adams et al. 2019) to explore the types of resulting information available from each and its relative value to the regulatory problem being studied. Basic statistical quantities6 are used to compare and contrast the various methods.\nLatin Hypercube Sampling\nMonte Carlo sampling is often considered the gold standard in forward UQ. However, for a high dimensional input space, there is risk of inadequately covering the sampling space. In general, it has been shown that Latin hypercube sampling (LHS) does a better job of covering the sampling space (see Adams et al. 2019, Chapter 5.2), therefore LHS has been used instead of Monte Carlo sampling in this study as a similar but more robust option. Since the computational cost of a single SCALE/MAVRIC calculation does not depend on the number of inputs when LHS is used, it will serve as the reference UQ analysis.\nPolynomial Chaos Expansion\nPolynomial chaos expansion (PCE) is a UQ method that has received some attention over the past several years in the nuclear engineering community among many others (Wu and Kozlowski (2017); Bledsoe, Jessee, and Knowles (2018); Williams (2007)). Attractive PCE characteristics are its high accuracy and cost-effective estimation of uncertainty.7 Additionally, PCE produces measures of input relative importance via Sobol sensitivity indices (Sobol 2001), and produces response sensitivity coefficients as a function of inputs for quick estimates of uncertainty. However, the main downside is that the number of simulations depends on the number of inputs and the desirable level of accuracy in the Gaussian quadrature procedure utilized in the method. Therefore, without further optimization, the particle transport simulation time practically limits the number of inputs that can be studied. Using a “hybrid method” such as is implemented in SCALE/MAVRIC, PCE becomes practical for the particle transport problem defined in this study. However, more efficient state-of-the-art particle transport methods in development now could allow for even larger scope UQ studies (Wang and Haghighat (2018); Walters, Roskoff, and Haghighat (2018)).\nProbability Bounds Analyis\nProbability bounds analysis (PBA) is the simplest of the UQ methods in that it’s objective is to find the global minimum and maximum of the response. This is useful to check if other UQ methods can actually approach “true” minimum and maximum values of the response. It can also be useful if a bounding analysis is desired. However, the current implementation of this (at least in DAKOTA) is computationally expensive because it implements an iterative search method that constrains SCALE-MAVRIC simulations to be run serially versus the LHS and PCE methods, which can be run in parallel.\nUQ Exploration Using Max Bioshield 0-Dimension Responses\nInputs Studied\nIt is important to first note that there are many contributors to the response uncertainty being studied here, but this study is focusing on the contribution of bioshield material uncertainties on response uncertainty. For example, reactor source term uncertainty has been completely left out. Additional uncertainty studies can be combined with work here, but again the point of this study is to highlight the relative importance of bioshield material uncertainties. Core water density uncertainty has been chosen here as a known dominant core-originating contributor to response uncertainty for comparison purposes to judge the importance of the biosheild material uncertainties.\nSpecific inputs of study that are included to better understand their importance are the thickness of the stainless steel liner that lines the concrete bioshield, the bioshield concrete density, and the weight percents of the materials that make up the concrete. Core soluble boron is also added to allow for an intuition check. Table 1 gives the inputs, means, standard deviation, and lower/upper bounds for sampling; all distributions were specified as Gaussian. Since most material information being sampled is not well characterized, we identify it as epistemic uncertainty. Modest standard deviations of 10% are specified for all but 2 inputs – core water density at 5% and the stainless steel liner thickness significantly more to obtain a more meaningful variation during sampling.8 While there may be some correlation between concrete density and concrete materials (and potentially others), this has been ignored, and all inputs are assumed to be independent. Since this study is more focused on relative importance and comparison of UQ methods, it was determined that the precise specification of input distributions are of less concern and reasonable simplifcations are therefore appropriate.\n\nTable 1: Input variable specifications.\ninput\nmean\nstdDev\nlwrBnd\nuprBnd\nh2oDens\n0.766\n0.03830\n0.6511\n0.88090\nppmB\n500.000\n50.00000\n350.0000\n650.00000\nconcDens\n2.280\n0.22800\n1.9380\n2.62200\nconcFe\n0.500\n0.05000\n0.3500\n0.65000\nconcC\n0.100\n0.01000\n0.0700\n0.13000\nconcSi\n8.500\n0.85000\n5.9500\n11.05000\nconcCa\n29.400\n2.94000\n20.5800\n38.22000\nconcK\n0.047\n0.00470\n0.0329\n0.06110\nconcAl\n0.530\n0.05300\n0.3710\n0.68900\nconcMg\n0.220\n0.02200\n0.1540\n0.28600\nconcNa\n0.200\n0.02000\n0.1400\n0.26000\nconcP\n0.200\n0.02000\n0.1400\n0.26000\nconcS\n0.200\n0.02000\n0.1400\n0.26000\nconcH\n0.600\n0.06000\n0.4200\n0.78000\nlinerR\n0.635\n0.42545\n0.0000\n1.91135\n\nBelow is a summary of all simulations showing the median results, distribution spread (50% of the data is in the white box), and the extremes.9 As expected, PBA produces the most extreme values. In terms of response coverage, LHS and PCE are similar, but PCE contains more spread. Although more spread is shown here, a higher standard deviation is not observed from the resulting PCE method polynomial expansion (refer to Table 2 below).\n\n\n\nFigure 1: Distribution of simulation responses by UQ method.\n\n\n\n\nTable 2: UQ method summary statistics.\nresponse\nuqType\nsamples\nmean\nstdDev\nrelStdDev\nwallTime\nmaxNeutronFluxE>0.1MeV\nlhs\n240\n1.66e+10\n3.56e+09\n0.2144578\n20200\nmaxNeutronFluxE>0.1MeV\nlhs\n120\n1.66e+10\n3.67e+09\n0.2210843\n7160\nmaxNeutronFluxE>0.1MeV\npce\n125\n1.66e+10\n3.61e+09\n0.2174699\n10500\nmaxPhotonDoseRate\nlhs\n240\n3.06e+04\n6.49e+03\n0.2120915\n20200\nmaxPhotonDoseRate\nlhs\n120\n3.08e+04\n7.46e+03\n0.2422078\n7160\nmaxPhotonDoseRate\npce\n125\n3.06e+04\n6.51e+03\n0.2127451\n10500\n\nLHS Reference Results\nLHS was conducted for 120 samples and 240 samples with little change in statistical moments.10 A look at the correlations quickly shows the most influential inputs.\n\nTable 3: Top four correlations with neutron flux.\ninput\nmaxN\nh2oDens\n-0.9396423\nconcDens\n-0.1630374\nlinerR\n0.1630147\nconcH\n-0.1406189\n\n\nTable 4: Top four correlations with gamma dose rate.\ninput\nmaxP\nh2oDens\n-0.8741187\nconcH\n0.1089407\nconcDens\n-0.0829845\nppmB\n-0.0528148\n\nFor both bioshield maximum incident neutron flux and gamma dose rate, core water density is clearly the most important input parameter. Concrete density, the bioshield liner, and concrete hydrogen content may also have some influence, but are seen to be relatively minor in comparison to core water density for both responses of interest. It should also be noted that such low values of the correlation coefficients can be misleading11 and are generally not useful (i.e., there is more noise than signal). A visual representation of the correlation structure in Figure 2 is also useful to get a sense of the input/output relationships and verify the numeric results.12 If one were to rank the 4 most influential input variables for this problem setup, one might choose core water density, concrete density, stainless steel liner thickness, and hydrogen content. Figure 2 shows strengths of correlations by distance and only draws lines where correlations exceed 0.3 giving a better impression of input influence – it is seen that concrete density, the stainless steel liner, and hyrogen content have little influence. This makes intuitive sense because the response of interest is at the concrete surface so the concrete material characteristics don’t yet have a chance to influence the responses; the liner is so thin that it’s influence is in the noise of the concrete concentration inputs. However, it is expected that the influence of the concrete material characteristics will change as the response location moves further into the concrete.\n\n\n\nFigure 2: Correlation plot.\n\n\n\nPCE Sobol Indeces and Sensitivity Coefficients\nComparing the 240 sample LHS case to the 125 PCE method case, the moments are seen to agree well providing confidence in the PCE method that only used the 3 most influential inputs of the 15 as shown in Table 1.\nSelecting the 4 most influential inputs based on insights from the LHS study, the PCE method is implemented; although 3 inputs (not including hydrogen concentration variation) produced acceptable moments, additional comparison of LHS correlation indications with PCE Sobol index indications was desirable. Again, due to an exponential increase in computation time with an increasing number of inputs specified, using PCE with some type of algorithmic pruning (if available) of lower order interactions ahead of time, to limit the number of simulations that will still produce reliable polynomial coefficients, is essential for computationally expensive simulations (e.g., see Yang, Xiong, and Wang (2017), Section 3); otherwise input variables must be manually chosen based on judgment. Given that it wasn’t clear from the LHS correlations alone which input variable is more important – stainless steel liner thickness or concrete hydrogen content – PCE is used to gain additional insight to determine if all 4 versus only 3 inputs are minimally necessary to capture the expected response variation.\nUsing the main Sobol indeces, the inputs and their interactions that are most significant in predicting the response of interest can be identified. Comparing the difference between the main and total Sobol indeces is useful to identify if there are any major input interactions. If looking at neutron flux as the response, it is seen that the main and total Sobol indeces for the core water density are practically the same and very close to 1 – they are 0.961 and 0.969, respectively. This means that there are no signficant input interactions with core water density. Also, being so close to 1 means that core water density can practically predict the max bioshield neutron flux without consideration of any other inputs.\n\n\n\nThe sensitivity coefficient was determined to be -9.15e+10 neutrons/cm2-s per gram/cm3 of core water density for the neutron flux and -2.49e+05 rad per second for the gamma dose rate; note that the signs match intuition – increasing the core water density decreases neutron and gamma flux (and drastically).13 An example of applying this sensitivity coefficient would be to check that the input standard deviation specified for core water density produces the standard deviation of the response. For core water density, the standard deviation of 10% is 0.0383 g/cm3. Multiplying this with the sensivitiy coefficients produces a reduction in neutron flux and gamma dose rate of 3.50e+09 and 9.54e+03, respectively. The neutron flux is similar to the actual response standard deviations given in Table 2 from the LHS study, but the gamma dose rate is noticeably different; -2% and 47%, respectively. It would be expected that the gamma dose rate is not as well explained by water density alone since the difference between its main and total Sobol indeces are not as close together as for the neutron flux – 0.788 and 0.939, respectively. Upon further inspection, of the input interactions, the following are observed: h2oDens * concDens (6.66%), h2oDens * linerR (2.31%), concDens * linerR (4.58%), and h2oDens * concDens * linerR (6.13%), which together add to the main Sobol index for a total sum of 98.5%. Using the corresponding polynomial coefficients, as shown in Table 5,14 reduces the difference between the reference LHS response standard deviation and that based on the PCE polynomial coefficents to within -14% versus 47% if using only the gamma dose rate core water density sensitivity coefficient. It is noted that including concrete hydrogen content as a 4th input does not result in any meaningful contribution to any of the Sobol indeces indicating that the only input variables that matter (of the ones studied here) for predicting maximum neutron flux and gamma dose rate incident on the concrete bioshield are core water density, bioshield concrete density, and bioshield stainless steel liner thickness.\n\nTable 5: Dominating PCE polynomial coefficients.\npolynomialCoeff\nh2oDens\nconcDens\nlinerR\nproduct\n30600\n1.0000\n1.000\n1.00000\n30600\n-150000\n0.0383\n1.000\n1.00000\n-5750\n-54300\n0.0383\n0.228\n1.00000\n-474\n25400\n0.0383\n1.000\n0.42545\n413\n63600\n0.0383\n0.228\n0.42545\n236\n\nIn summary, UQ with PCE compares well to UQ with LHS as long as the important input variables are included for polynomial coefficient development so some knowledge of input importance must be known a priori with this manual approach. PCE has the advantage of providing sensitivity coefficients for local uncertainty estimation, Sobol indeces to judge input importance, and polynomial coefficients related to each input variable and all interactions to serve as a surrogate model for future use (e.g., further forward UQ studies).\nPBA Convergence\nThe PBA method serves as a useful tool for bounding analysis in support of a conservative approach to UQ as seen by the sheer number of simulations needed for this type of analysis versus LHS and PCE – 552 and 869 cases if optimizing on the neutron flux and gamma dose rate, respectively.15 Additionally, the majority of these cases are run serially rather than in parallel making this method take even longer per simulation. It should be noted that all 15 input variables were included in the global searches.\nThe log scale of Figure 3 accentuates the wild swings in the “expected improvement” of the “NCSU Direct” explore/exploit global minimum/maximum solver (see pg. 134 of Adams et al. (2019)). The responses, however, do not change much on this scale, but a characteristic jump is observed for both responses. The evolution of the response is shown more clearly in Figure 4.\n\n\n\nFigure 3: PBA method “NCSU Direct” solver convergence.\n\n\n\n\n\n\nFigure 4: PBA method “NCSU Direct” solver response evolution.\n\n\n\nComparing the extreme values of the various methods in Table 6 (also shown visually in Figure 1), it is seen that if the LHS maximum is used for a bounding analysis, it would underpredict the maximum neutron flux found by PBA by about 33% and the maximum gamma dose rate by about 47%.\n\nTable 6: UQ method extreme responses.\nresponse\nuqType\nsamples\nmin\nmax\nwallTime\nmaxNeutronFluxE>0.1MeV\nlhs\n240\n8.88e+09\n2.83e+10\n20200\nmaxNeutronFluxE>0.1MeV\nlhs\n120\n9.23e+09\n3.15e+10\n7160\nmaxNeutronFluxE>0.1MeV\npce\n125\n8.83e+09\n3.10e+10\n10500\nmaxNeutronFluxE>0.1MeV\npba\n552\n6.70e+09\n4.20e+10\n340000\nmaxPhotonDoseRate\nlhs\n240\n1.78e+04\n5.17e+04\n20200\nmaxPhotonDoseRate\nlhs\n120\n1.77e+04\n5.31e+04\n7160\nmaxPhotonDoseRate\npce\n125\n1.61e+04\n5.30e+04\n10500\nmaxPhotonDoseRate\npba\n869\n1.23e+04\n9.79e+04\n665000\n\nThe values of the 15 inputs that produced the minimum/maximum values are also provided as part of the output, but are not shown here. This may be useful to help determine if minimizing/maximizing a given input variable would lead to a decrease/increase in a response of interest.\nUQ Exploration Using Max Bioshield 1-D Responses\nTo assume all of the bioshield concrete is exposed to the maximum amount of neutron fluence and gamma dose is overly conservative for estimating how much of the concrete will be degraded by the end of plant life. Therefore, understanding how far the radiation “penetrates” into the bioshield with respect to an assigned damage threshold – i.e., the damage depth – is of some merit.\nBased on the insights gained from the 0-D UQ study above, the UQ methods used are limited to LHS and PCE. Based on the performance of PCE from the 0-D study, it is of interest to see if PCE again provides similar results to LHS, but throughout the bioshield. To help understand the sensitivity of the response uncertainty to the inclusion of different numbers of inputs, 3 LHS cases were included: 1 input (core water density), 3 inputs (core water density, concrete density, and liner thickness), and all 15 inputs. Comparing these cases will give some idea of the relative importance of the various inputs. An additional case using PCE is also added with the same 3 input variables as the LHS case.\nResponses with Uncertainties Through Bioshield\n\n\n\nFigure 5: Axial maximum radial response throughout bioshield.\n\n\n\nThe mesh tally is defined at the peak azimuthal location corresponding to the HBR-2 benchmark (a couple degrees above the core centerline to account for presence of detector well).\nMonte Carlo errors were extracted for each given mesh tally cell.\nUncertainties Through the Bioshield\n\n\n\nFigure 6: Axial maximum relative uncertainty throughout bioshield.\n\n\n\nMain Observations\nThe main goal of this study was to get a general idea of how much uncertainty the concrete adds to total response uncertainty. The main takeaways are:\nCore based uncertainties will dominate total response uncertainty up to concrete depths corresponding to radiation damage thresholds, but will eventually switch over to concrete uncertainties dominating significantly past the damage depth. This is easiest to see by examining Figure 6 1-sigma uncertainties between lhs1Var and lhs3Var16 cases (they start out similarly) and in Figure 7 below.\nThe independence of core- and concrete-based uncertainties can be used to separate the core-based uncertainties in lhs1Var from core- plus conrete-based uncertainties in lhs15Var via root-mean-sum-of-squares as was done in Figure 7.\n\n\n\nFigure 7: Axial maximum relative uncertainty throughout bioshield.\n\n\n\nUncertainties up to concrete are likely less than 20-30%17 for most methods at the axial maximum location (water density alone showed a ~20% 1-sigma from lhs1Var in Figure 6) and remain roughly constant throughout the concrete as seen in the lhs1Var case in Figure 6. Concrete-based uncertainties can be added directly to core-based uncertainties in future studies to estimate core- plus concrete-based uncertainties throughout the wall.\nUncertainty is mostly captured by the 3 input models versus the 15 input model; however, maximum uncertainty – for the neutron case only – is not; this may be unimportant because the response is so low, but allows for understanding of relative contribution to overall response uncertainty.\nAgain, moments derived from PCE samples are shown, but are not the same as the polynomial coefficient derived moments. Nonetheless, the polynomial coefficient derived red 1-sigma line for the pce3Var case is similar to but larger than the corresponding lhs3Var case.\nSobol indeces from the pce3Var case provides a numerical basis for the influence of input variables on a given response as seen in Figure 8. The inherent advantage of Sobol indeces as shown in Figure 8 versus correlation coefficients provided in Figure 2 and corresponding tables from LHS in terms of interpretability is noted.\n\n\n\nFigure 8: Axial maximum Sobol indeces throughout bioshield.\n\n\n\nRadiation Damage Depth Based on the Maximum Uncertainty\nTo get an idea of the variation in predicted radiation damage depth between UQ methods, the maximum uncertainty resulting from the implementation of each method studied was calculated. Looking only at the predicted maximum depths on an absolute basis (maxUnc column of Table 7), all methods give very similar results. The outcome of the study indicates underprediction in neutron damage depth of around ~5 cm and ~14 cm for gamma damage depth. These are plausible results, but are based on a limited set of core-based uncertainty and do not account for any nuclear data uncertainty. However, maximum flux/dose uncertainties were used to determine the damage depth uncertainty in this case, which may somewhat compensate for this. Nonetheless, the uncertainty and input importance information gleaned above can be used in future studies of this problem.\n\nTable 7: Damage depth by UQ method and radiation type.\ntype\nstudy\nBE\nmaxUnc\nabsDiff\nrelDiff\nneutron\nlhs15Var\n7.58\n12.62855\n-5.048547\n-0.6660352\nneutron\npce3Var\n7.58\n12.32498\n-4.744977\n-0.6259865\nneutron\nlhs3Var\n7.58\n11.60333\n-4.023331\n-0.5307825\nneutron\nlhs1Var\n7.58\n10.49239\n-2.912386\n-0.3842198\nphoton\npce3Var\n17.18\n30.82339\n-13.643393\n-0.7941440\nphoton\nlhs15Var\n17.18\n29.75828\n-12.578284\n-0.7321469\nphoton\nlhs3Var\n17.18\n29.73675\n-12.556753\n-0.7308937\nphoton\nlhs1Var\n17.18\n26.49821\n-9.318210\n-0.5423871\n\nConclusions and Future Study\nThe regulatory and research objectives outlined above have been achieved. Although the selection of core- and concrete-based input uncertainties were not exhaustive, for the coupled neutron-gamma transport problem defined in this study under the current regulatory framework, core-based uncertainties18 clearly dominate and should be accounted for. A 20-30% underestimation in concrete damage (derived from lhs1Var results in Table 7) was observed assuming a single core-based uncertainty (i.e., core peripheral water density) is not accounted for; an additional 15-20% underestimation (derived from lhs15Var minus lhs1Var results in Table 7) was observed if the assumed concrete-based uncertainties are not accounted for. These uncertainty estimates pertain to a specific reactor and surrounding structures model, but it is expected that similar results could be expected for other reactor and surrounding structures models in the vicinity of the core midplane.\nIt is well known that nuclear data uncertainties can be significant contributors to response uncertainty in radiation transport problems (Cabellos et al. (2014); Diez et al. (2015)). Recently, in SCALE Version 6.2, the SAMPLER module has been released allowing any SCALE calculation to incorporate nuclear data uncertainty (Wieselquist 2017). Therefore, it is of interest to update the current study with a follow-on study including nuclear data uncertainty to examine the effect on the uncertainty results. Additionally, since flux/dose calculations are fed into downstream stress calculations, it may be of interest to extend this study to consider 2-D or 3-D analyses, and to better understand the flux/dose impacts on the actual stress response.\n\n\n\nAdams, Brian M., Michael S. Eldred, Gianluca Geraci, Russell W. Hooper, John D. Jakeman, Kathryn A. Maupin, Jason A. Monschke, et al. 2019. Dakota, a Multilevel Parallel Object-Oriented Framework for Design Optimization, Parameter Estimation, Uncertainty Quantification, and Sensitivity Analysis: Version 6.9.0 User’s Manual. Sandia National Laboratories.\n\n\nBledsoe, Keith C, Matthew Anderson Jessee, and Justin R Knowles. 2018. “Application of Polynomial Chaos Expansion in Inverse Transport Problems with Neutron Multiplication Measurements and Multiple Unknowns.” In Proceedings of the 20th Topical Meeting of the Radiation Protection & Shielding Division of ANS, Sante Fe, NM, USA. LaGrange Park, IL: Oak Ridge National Lab, Oak Ridge, TN; American Nuclear Society.\n\n\nCabellos, O, E Castro, C Ahnert, and C Holgado. 2014. “Propagation of Nuclear Data Uncertainties for PWR Core Analysis.” Nuclear Engineering and Technology 46 (3): 299–312.\n\n\nDiez, Carlos Javier, Oliver Buss, Axel Hoefer, Dieter Porsch, and Oscar Cabellos. 2015. “Comparison of Nuclear Data Uncertainty Propagation Methodologies for PWR Burn-up Simulations.” Annals of Nuclear Energy 77: 101–14.\n\n\nField, Kevin G, I Remec, and Y Le Pape. 2015. “Radiation Effects in Concrete for Nuclear Power Plants–Part i: Quantification of Radiation Exposure and Radiation Effects.” Nuclear Engineering and Design 282: 126–43.\n\n\nNRC. 2001. “Regulatory Guide 1.190, Calculational and Dosimetry Methods for Determining Pressure Vessel Fluence.” https://www.nrc.gov/docs/ML0108/ML010890301.pdf.\n\n\n———. 2013. “NUREG/CR-7171, a Review of the Effects of Radiation on Microstructure and Properties of Concretes Used in Nuclear Power Plants.” https://www.nrc.gov/docs/ML1332/ML13325B077.pdf.\n\n\n———. 2017. “NUREG-2192, Standard Review Plan for Review of Subsequent License Renewal Applications for Nuclear Power Plants - Final Report.” https://www.nrc.gov/docs/ML1718/ML17188A158.pdf.\n\n\nRearden, B. T., and M. A. Jessee, eds. 2018. SCALE Code System, ORNL/TM-2005/39, Version 6.2.3. Oak Ridge, Tennessee: Oak Ridge National Laboratory.\n\n\nRemec, I., T. M. Rosseel, K. G. Field, and Y. Le Pape. 2018. “Radiation-Induced Degradation of Concrete in NPPs.” In Proceedings of the 16th Int. Symposium on Reactor Dosimetry, Sante Fe, NM, USA, Astm1608, edited by M. H. Sparks, K. R. DePriest, and D. W. Vehar, 201–11. West Conshohocken, PA: ASTM International. https://doi.org/http://dx.doi.org/10.1520/STP160820170059.\n\n\nSobol, Ilya M. 2001. “Global Sensitivity Indices for Nonlinear Mathematical Models and Their Monte Carlo Estimates.” Mathematics and Computers in Simulation 55 (1-3): 271–80.\n\n\nTyson, S, D Donovan, B Thompson, S Lynch, and M Tas. 2015. “Uncertainty Modelling with Polynomial Chaos Expansion: Stage 1–Final Report.”\n\n\nWagner, John C, Edward D Blakeman, and Douglas E Peplow. 2009. “Forward-Weighted CADIS Method for Variance Reduction of Monte Carlo Calculations of Distributions and Multiple Localized Quantities.” In Proceedings of the 2009 Int. Conference on Advances in Mathematics, Computational Methods, and Reactor Physics, Saratoga Springs, NY, USA.\n\n\nWalters, William J, Nathan J Roskoff, and Alireza Haghighat. 2018. “The RAPID Fission Matrix Approach to Reactor Core Criticality Calculations.” Nuclear Science and Engineering 192 (1): 21–39.\n\n\nWang, Meng-Jen, and Alireza Haghighat. 2018. “A Novel Detector Response Formulation for Rapid.” In Proceedings of PHYSOR.\n\n\nWieselquist, Will. 2017. “Uncertainty in Anything: The Future with SCALE/Sampler.” Presentation. In 2017 SCALE Users’ Group Workshop. https://www.ornl.gov/scale/scale/2017-scale-users-group-workshop.\n\n\nWilliams, M. M. R. 2007. “Polynomial Chaos Functions and Neutron Diffusion.” Nuclear Science and Engineering 155 (1): 109–18. https://doi.org/10.13182/NSE05-73TN.\n\n\nWu, Xu, and Tomasz Kozlowski. 2017. “Inverse Uncertainty Quantification of Reactor Simulations Under the Bayesian Framework Using Surrogate Models Constructed by Polynomial Chaos Expansion.” Nuclear Engineering and Design 313: 29–52.\n\n\nYang, Shuxing, Fenfen Xiong, and Fenggang Wang. 2017. “Polynomial Chaos Expansion for Probabilistic Uncertainty Propagation.” In Uncertainty Quantification and Model Calibration. IntechOpen.\n\n\nFor example, the presence of a stainless steel liner, concrete density, and concrete material composition.↩︎\nThat is, with respect to the damage thresholds defined in SRP-SLR, Section 3.5.2.2.2.6.↩︎\nMore specifically, special care in the selection of a suitable multigroup nuclear data library is not applicable, and space-angle meshing selection and companion studies justifying their suitability are not applicable.↩︎\nIntel Xeon Platinum 8175M processor with a 16 vCPU core clock speed of up to 2.5 GHz and 64 GiB of memory.↩︎\nThe reactor vessel dimensions and base concrete material model were adapted to match Turkey Point Unit-3,4 as part of the confirmatory studies supporting 80 year license renewal.↩︎\nThat is the mean, spread, and extremes of the response distributions.↩︎\nTyson et al. (2015) provides a clear and concise overview of PCE with additional references.↩︎\nThe mean thickness is 1/4 inch so the selected variation allows for as low as 0 inches to approximately 3/4 inch.↩︎\nOnly for LHS does the boxplot represent actual statistical moments (i.e., mean, variance). That is, the PCE method statistical moments are determined using the polynomial expansion and the PBA method doesn’t give moment information, only estimated global minimum and maximum response values.↩︎\nThe choice of samples was initially based on using the Wilks formula for determining the number of samples to take for 95% population coverage at 95% confidence for a single response (i.e., 59) multiplied by 2.↩︎\nFor example, the correlation for neutron flux and stainless steel liner lining the concrete bioshield in Table 3 shows as a positive relationship (0.163) when it is known that this relationship is negative. That is, a thicker liner will cause the neutron flux to decrease.↩︎\nThe observable correlation between oxygen concentration and calcium concentration is an artifact of the model setup as the oxygen content was not sampled but simply defined as the balance of the material after all other concrete elemental concentrations (11.1 wt% on average) were sampled. Since calcium (29.4 wt% on average) is the next largest elemental concentration after oxygen (59.5 wt% on average), there is a more clear and detectable linkage between these 2 variables and this is what is observable in the correlation figure.↩︎\nNote that the neutron flux sensitivity coefficient for the stainless steel liner thickness is negative (-1.22e+09 neutrons/cm2-s per cm of liner thickness) as expected – recall this relationship showed as positive when looking at the corresponding correlation from the LHS method discussed previously.↩︎\nThe first column represents the polynomial coefficient for the row. A 1 in columns 2-4 represents a zeroth order contribution of the input variable, which is indicated by the table header. A number other than 1 means that there is a first order term; multiple first order terms in a row signifies a signficant interaction. The 5th column represents the product of Columns 1-4. The sum of Column 5 produces the estimated response.↩︎\nIt appears that multiple responses cannot be treated simultaneously in DAKOTA, therefore 2 separate PBA cases – one to find the neutron flux maximum/minimum and one for the gamma dose rate maximum/minimum was needed.↩︎\nThe difference between lhs3Var and lhs15Var is minimal as seen in Figure 5.↩︎\nThis is largely consistent with NRC approved calculational neutron fluence methodologies for estimates near the reactor pressure vessel core midplane.↩︎\nSee NRC (2001).↩︎\n",
    "preview": "papers/2019-05-15-bepu-ce-mc-n-p-transport-3loop-bioshield/bepu-ce-mc-n-p-transport-3loop-bioshield_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-01-30T15:53:27+05:45",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 537
  },
  {
    "path": "papers/2018-10-01-atf-sfp-csa-validation/",
    "title": "Criticality Code Validation of Spent Fuel Pool Criticality Safety Analyses for ATF Fuel System Concepts with Greater Than 5% Enriched Uranium",
    "description": "Licensing proposed ATF fuel concepts under the current regulatory framework will likely require the adoption of state-of-the-art criticality safety validation techniques, which may result in a significant reduction in operating margin.",
    "author": [
      {
        "name": "Amrit Patel",
        "url": {}
      }
    ],
    "date": "2018-10-01",
    "categories": [],
    "contents": "\n\nContents\nExecutive Summary\nIntroduction\nPrevious Research\nExperimental Benchmarks\nCase Studies\nConclusions\n\nCurrent Research\nCase Studies\nModels\nExperimental Benchmarks\nck Trending\nExperimental Benchmark Coverage\n\nConclusions\n\nFinal Thoughts\n\nExecutive Summary\nLicensing proposed ATF fuel concepts under the current regulatory framework of 10 CFR Part 50 with respect to criticality safety of SFP storage, while challenging, is not impossible. However, this will likely require the adoption of state-of-the-art criticality safety validation techniques, which may result in a significant reduction in operating margin.\nIntroduction\nThe latest draft of the Accident Tolerant Fuel (ATF) action plan [NRC (2017)“] identified Open Item 1 to assess the need for regulatory guidance to accommodate licensing ATF designs under the current regulatory framework. Related activities include determining/clarifying the regulatory criteria that need to be satisfied for partial/full core use of ATF and regulatory options available to licensees, applicants, and vendors. Table 2,”Regulatory Framework, In-reactor Performance,\" of the draft action plan identifies coated zirconium cladding and FeCrAl cladding as evolutionary ATF concepts, while silicon-carbide cladding, uranium silicide fuel, metallic fuel, and any fuel with greater than 5% enriched uranium as revolutionary concepts. Table 2 also notes that rulemaking or exemptions to 10 CFR 50.68 will be likely for designs containing uranium above 5 weight percent (wt%) uranium-235 (U-235).\nThe objective of this scoping study is to evaluate potential licensing challenges with respect to spent fuel pool (SFP) storage criticality safety reviews where 10 CFR 50.68 would typically be applicable (i.e., U-235 enrichment less than 5 wt%). Case studies considering both evolutionary and revolutionary candidate ATF fuel system design concepts currently being discussed in the open literature and exceeding 5 wt% U-235 enrichment are evaluated.\nPrevious Research\nIn 2008, studies were conducted by the University of Tennessee to evaluate and extend the experimental basis for licensing 5-10 wt% U-235 nuclear reactor fuel (Barnes, King, and Pevey 2008). The objective of the two-year project was to examine the need for critical experiments in the 5-10 wt% U-235 enrichment range in support of the licensing of nuclear reactor uranium fuel production. Because the focus of this research was the fuel production cycle (uranium hexfluoride gas container shipping, uranium dioxide powder production, uranium dioxide finished fuel pellet storage, and fuel reactor assembly cask shipping),1 and since the studies are limited to 10 wt% U-235 enrichment, the majority of the work is not necessarily directly applicable to SFP storage with the types of ATF fuel systems of interest in the present study.\nExperimental Benchmarks\nThe International Handbook of Evaluated Criticality Safety Benchmark Experiments (IHECSBE or “handbook”) was used to identify potentially applicable evaluated benchmark experiments of interest. The majority of benchmark experiments contained in the handbook have corresponding sensitivity data files (SDFs) that are also included along with the handbook. The SDFs are output from SCALE/TSUNAMI-3D (ORNL 2009) calculations of a modeled system of interest, and essentially contain experimental “signature” data that allows for comparison to other signature data based on a calculational model of the application of interest. Comparison of the SDF signature files, through the ck2 parameter allows for similarity assessment between experiment and application using a single integral parameter. A ck value of 1 indicates perfect physical similarity and a value of 0 indicates no physical similarity (Broadhead et al. 2004). Therefore, use of the ck parameater via SDFs allows for a convenient screening parameter to assess the availability of validation data for a given application.\nCase Studies\nSeveral calculational cases were defined and SDFs were generated by the researchers to represent the various applications of interest. The applicability of each of the experiments to each of the cases was evaluated based on the ck parameter following previously published Oak Ridge National Laboratory (ORNL) guidance on recommended ck screening values.\nConclusions\nBuilding on previous research, it was determined that experimental benchmark coverage would be deemed adequate as long as there were 25 experiments observed to have ck greater than or equal to 0.80 or 15 experiments with ck greater than or equal to 0.90. The researchers concluded that the evaluation indicates that existing experiments generally provide adequate coverage of the base cases examined stating:\n\nFive out of the seven configurations analyzed were shown to meet at least one of the criteria, leading to an assessment that they are adequately covered by the subset of experiments used. The two remaining cases–representing dry UF6 canisters at 5-10% 235U enrichment and bulk damp powder cases above about 8% 235U enrichment–are predicted to be adequately covered in true validation cases for which a more extensive subset of the existing critical experiments would be expected to be utilized. This result depends heavily on the second criteria; most of the cases fail to identify enough experiments with ck > 0.90, so if this criterion becomes more important, the results of this project will have to be reevaluated.\n\nCurrent Research\nIt should be noted that the case study results discussed below are specific to those systems modeled and generalizations should not be made without careful thought. The intent of these scoping studies is to assess the need for better experimental coverage given the current snapshot of potential ATF fuel system concepts. These studies are also intended to illustrate a modern framework for criticality safety analysts to make informed decisions about criticality code validation when assessing new systems.\nCase Studies\nAs discussed in the ATF action plan, there is potential interest in pursuing greater than 5 wt% U-235 enriched fuel for some of the ATF fuel concepts. For the evolutionary concepts, this includes enrichments only slightly greater than 5 wt% U-235. As an example of an evolutionary fuel cladding, if iron-cromium-aluminum (FeCrAl) cladding is selected for use in a pressurized water reactor (PWR), this would require a U-235 wt% enrichment of approximately 5.4-6.3 wt% without having to significantly change fuel bundle design parameters such as fuel pin pitch or fuel pin outer diameter (OD) for the current standard 17x17 PWR fuel assembly (Eckleberry and Maldonado (2016); Shapiro, Younker, and Fratoni (2013)).\nA revolutionary concept could include much higher U-235 enrichments depending on the specific fuel pin design. One example that appears frequently in the open literature is the fully ceramic micro-encapsulated (FCM) fuel concept, which consists of TRISO particles dispersed in a silicon-carbide (SiC) matrix to form fuel pellets. The TRISO particles themselves could contain a variety of uranium-based fuels, but in this example, uranium silicide is selected as it is a candidate fuel discussed in the ATF action plan. For this same reason, the SiC based FCM fuel design was selected. In this case, in order to maintain the current standard 17x17 PWR fuel assembly dimensions, as with the previous case, the U-235 enrichment must be allowed to vary. However, to compensate for the fact that the heavy metal loading of FCM fuel is realtively low due to the TRISO particles having a packing fraction of about 45%, the pin OD must also be able to be varied. This is mechanically possible for a current standard 17x17 PWR fuel assembly assuming a pin OD of about 1.1 cm, however this also requires a U-235 enrichment of almost 20% (Shapiro, Younker, and Fratoni 2013).\nModels\nAs with the previous research, application-specific TSUNAMI-3D models were created. In the current study, two semi-infinite (assuming no axial effects) TSUNAMI-3D models were created simulating a SFP environment using SCALE 6.2. The basic characteristics include a 17x17 PWR fuel assembly with the selected ATF concept surrounded by a conventional stainless steel storage rack structure. The models (somewhat arbitrarily)3 assume no fixed neutron absorber material (NAM) as part of the rack and water with 1000 parts per million (ppm) of soluble boron.\nAfter the two TSUNAMI-3D models were run, SDF files were output for use with TSUNAMI-IP. The kinf of the uranium dioxide (UO2) (5.86 wt% U-235) with FeCrAl cladding case was observed to be ~1.24, which is similar to an expected conventional peak light water reactor fuel lattice kinf(Smith et al. 2017). The kinf of the uranium silicide (U3Si2) (20 wt% U-235) in a SiC matrix with SiC cladding case was observed to be ~1.48, which is close to what is expected for this type of fuel in the conventional PWR fuel assembly geometry (George et al. 2014).\nExperimental Benchmarks\nSince the previous 2008 study was conducted, there have been several updates to the IHECSBE handbook. Although many potentially useful experiments have been added since 2008,4 the corresponding SDFs have not been included with distribution of the handbook, and therefore unfortunately have not been included in this study. Needless to say, future development of TSUNAMI-3D models for these experimental series may be useful to support greater than 5 wt% U-235 enrichment validation studies related to ATF fuel concepts.\nAs with previous research, TSUNAMI-IP was run with the two ATF fuel concept case studies as the applications of interest. The SDFs corresponding to the benchmark experiments provided as part of the latest release of the handbook were used as the experiments to be used for ck screening. Although 4316 SDFs were included with the handbook, only 3984 were found to work with TSUNAMI-IP – likely due to formatting errors. Nonetheless, running TSUNAMI-IP for the two ATF fuel concept applications resulted in 315 experiments with a ck greater than or equal to 0.80 for the UO2-FeCrAl case and 566 experiments with a ck greater than or equal to 0.80 for the FCM-SiC case. Although this indicates that there are relatively few publicly available benchmark experiments that are relevant to the selected ATF fuel concepts in the context of the theoretical SFP criticality safety analysis being performed, the criteria defined in the previous study that greater than 25 experiments with a ck of greater than or equal to 0.80 should be used as a minimum experimental validation set has been met. It is likely that future consideration of LEU-COMP-THERM-096, LEU-COMP-THERM-097, and LEU-COMP-THERM-098 would increase the number of applicable experiments.\nIt is notable that in the current study, the number of experiments exceeding a ck of 0.90 is zero, whereas in the previous 2008 study, several were found for the similar flooded transport cask cases (although this case did not contain soluble boron). Furthermore, hundreds of experiments were found exceeding a ck of 0.80 for the flooded transport cask cases. A potential cause of the significant reduction in number of applicable critical experiments across relatively similar applications studied in 2008 and now in 2018 could be due to over-estimation of ck due to lower fidelity nuclear covariance libraries used in the earlier versions of SCALE (Marshall et al. 2015). The main implication, however, is that the previous 2008 study should be re-evaluated using the latest nuclear data as experimental coverage conclusions may change. Nevertheless, the same ck evaluation guidelines remain valuable and were used in the present study to assess experimental benchmark coverage for SFP storage of potential greater than 5 wt% U-235 ATF fuel system concepts. It is also noted that SCALE Version 6.2 (Rearden and Jessee 2016) is used in the current study, which contains the highest fidelity covariance data available to-date.7\nck Trending\nTraditional validation techniques (see Section 6.1 of NUREG/CR-7109) may be used, but require significant engineering judgment (Scaglione et al. 2012). ck trending is preferable as it is an objective method based on the physical application under consideration and removes engineering judgment. Specifically, the idea is to make sure that the experimental validation set is weighted by those experiments with the largest important sources of uncertainty for the application of interest. Therefore, any bias and bias uncertainty derived from these experiments should allow for the most physically representative bias possible based on the experiments available assuming that nuclear data uncertainties are expected to dominate any true computational bias and bias uncertainty.\nck trending has been documented in NUREG/CR-7109 (see discussion on pg. 19 and Section 7.4.2). It is noted that NUREG/CR-7109 recommends ck trending only if there are a significant number of experiments with ck values greater than or equal to 0.90 because “the interpolation would be over too large a range.” However, relaxation of this criterion to 0.80 is a compromise between using traditional trending techniques involving significant engineering judgment and the more physically-based ck trending method. In other words, while this may provide for more interpolation than is desirable, it is still a more physically comprehensive method than using engineering judgment to pick relevant individual parameters, which are by definition less comprehensive, to trend on.\nUO2 (5.86 wt% U-235) with FeCrAl Cladding\nIt is important to note that the underlying benchmark experiment data with ck greater than or equal to 0.80 summarized in the histogram below did not pass the Chi-Squared test for normality.\n\n\n\nThis makes any trending analysis questionable. The data in this case is seen to be excessively peaked around ~0.998. Additionally, it is noted that there are a relatively few number of measurements, which can be problematic for normality testing. Finally, upon visual observation of the data, there aren’t any obvious features that would imply that assuming normality would be problematic in terms of non-conservatively biasing uncertainty estimates. In this case, the data will be treated as normal for the sake of discussion.8\n\nAt the trending parameter (i.e., ck) value of 1, the average experimental keff, denoted by the “k(x)” curve, is observed to extrapolate to less than 1, which is indicative of a keff underprediction bias. Therefore, this difference between keff equal to 1 and the average experimental keff would be accounted for as part of the criticality safety analysis validation. In this case, the bias was determined to be 585.2 pcm. The corresponding uncertainty based on the single sided lower tolerance limit, denoted by the “k(x) - w(x)” curve, is 2061.7 pcm.\nU3Si2 (20 wt% U-235) in a SiC Matrix with SiC Cladding\nAgain, it is important to note that the underlying data shown did not pass the Chi-Squared test for normality, and the same conclusions apply and are not repeated. It is noted that one difference in this case is an apparent skewness in the data, which could be a significant feature that would warrant further inspection – especially since the data is skewed in the non-conservative prediction direction (i.e., keff overestimation).\n\n\n\n\nRepeating the same exercise as with the previous case, the average experimental keff at a ck value of 1 is observed to extrapolate to greater than 1, which is indicative of a keff overprediction bias. Since it is typical to not credit overprediction bias in criticality safety analysis validation (ANS 2017), the bias would be reported as 0 pcm. However, the bias uncertainty is still included and is observed to be 2634 pcm.\nNotes on Generalized Linear Least-Squares Method (GLLSM) Bias and Bias Uncertainty Estimates\nBased on the ck trending results in the preceding subsection, one might argue that there should be more relevant experiments (i.e., more experiments in general; more/some with ck greater than 0.90; more at higher enrichments) in support of the validation effort. However, if fuel systems used in commercial nuclear power plants do actually evolve into those similar to the ones being considered in this study, the criticality safety community will most certainly encounter shortages of relevant criticality safety experiments for users’ specific applications.\nRecent research also provides limited evidence that use of ck trending using too few experiments could lead to a decrease in an ability to reliably predict bias estimates (Perfetti, Rearden, and Marshall 2018). The same research indicates that a GLLSM, like the one implemented in the SCALE/TSURFER tool, could provide a means for more reliable bias estimates based on the observation that the standard deviation of the TSURFER bias estimates was lower than that from the ck trending bias estimates; however, this will require additional study.\nIt is also important to note that use of TSURFER for criticality code validation is attractive because the GLLSM implemented in TSURFER does not rely upon the availability of highly similar experiments to estimate computational bias for a given application. Additionally, recent research has addressed many of the concerns raised regarding use of codes like TSURFER for validation making them even more attractive (Perfetti and Rearden 2018).9\nExperimental Benchmark Coverage\nUO2 (5.86 wt% U-235) with FeCrAl Cladding\nBased on TSUNAMI-IP analysis, the total relative standard deviation of keff due to uncovered sensitivity data is ~100 pcm/k. Studies have shown that using three times this relative standard deviation (3-sigma) applied as a bias may provide for a conservative “coverage penalty” (Scaglione et al. 2012).\nThe dominant sources of lack of coverage (i.e., ~95%), which dominate the coverage penalty, come from two reactions: the Fe-56 (n, gamma) capture reaction and the U-238 (n, n’) reaction. Therefore, one could conclude that additional experiments to better cover these two reactions would significantly increase overall experimental coverage leading to experiments with higher cks and less of a coverage penalty.\n\nTwo sensitivity profiles are shown in the figure above for the reaction contributing most to the coverage penalty. In this case it is clear that there aren’t sufficient benchmark experiments containing Fe-56 sensitive to the thermal region of the spectrum between 0.01 and 0.1 eV.\nThe SCALE manual also describes the TSUNAMI-IP completeness parameter, which can be used to:\n\nAssess the completeness of a set of experiments for the code validation of a given application. The set of experiments is “complete” in the sense that it completely tests all the important cross-section elements in the particular application of interest.\n\nFor the UO2-FeCrAl cladding case study, the completeness parameter was determined to be 0.7761 based on the 31 handbook benchmark experiments exceeding or equal to a ck value of 0.80. This single summary parameter is therefore useful to gauge general experimental coverage and can serve as a guide to code practitioners as to whether or not more and/or better experiments should be considered.\nU3Si2 (20 wt% U-235) in a SiC Matrix with SiC Cladding\nBased on TSUNAMI-IP analysis, the total relative standard deviation of keff due to uncovered sensitivity data is again ~100 pcm/k. However, the absolute impact is larger because keff is larger for this system versus the UO2-FeCrAl system (i.e, ~1.48 vs. ~1.24, respectively), and consequently the coverage penalty would be larger (i.e., ~450 pcm vs. ~375 pcm). In this case, the dominant sources of lack of coverage (i.e., ~95%) come from four reactions: Si-28 (n, gamma), Si-28 (n, p), Si-30 (n, gamma), and Si-28 (elastic).\n\nTwo sensitivity profiles are shown in the figure above for the reaction contributing most to the coverage penalty. In this case it is clear that there aren’t sufficient benchmark experiments containing Si-28 sensitive to the thermal region of the spectrum between 0.01 and 10 eV and for parts of the fast region for energies greater than ~5 MeV.\nFor this case the completeness parameter was determined to be 0.7726 based on the 56 handbook benchmark experiments exceeding or equal to a ck value of 0.80. As before, one could conclude that additional experiments to better cover these four reactions would significantly increase overall experimental coverage leading to experiments with higher cks and less of a coverage penalty.\nConclusions\nUsing the TSUNAMI tools (or tools like them), the following high level strategy could be used for criticality code validation based on adequate experimental coverage:\nPerform ck trending analysis.10 Determine code bias and bias uncertainty at ck = 1 if using the single sided lower tolerance limit approach.\nDetermine an appropriate coverage penalty.11\nIf Step 1 is unfeasible (or excessively conservative) or the coverage penalty is determined to be too large (or excessively conservative), determine the TSUNAMI-IP completeness parameter. Try and find more applicable experiments, re-run TSUNAMI-IP incorporating any newly identified experiments, and verify an increase in the completeness parameter. Return to Step 1. If no additional experiments can be found or Step 1 remains unfeasible, use two to three times the problem-specific nuclear data uncertainty, applied as a bias, to conservatively bound the code bias and bias uncertainty (see pg. 13 of NUREG/CR-7109).\nFinal Thoughts\nThis scoping study indicated that there are experimental benchmark data and advanced analysis tools that could allow for defensible criticality code validation for a variety of evolutionary and revolutionary ATF fuel systems. Since criticality code validation is very problem-specific, licensees, applicants, and vendors will need to conduct their own studies to support ATF licensing (e.g., if seeking an exemption from 10 CFR 50.68 due to use of greater than 5 wt% U-235 enrichment). It is also likely that these studies will also include credit for the reactivity decrease associated with depleted fuel (known as burnup credit), which introduces the added complexity of having to validate minor actinides and fission products. However, use of the TSUNAMI and TSURFER tools and other tools like them, have been effectively used to overcome validation challenges and it is foreseen that this will continue as nuclear fuel systems evolve and outpace the creation of additional and costly relevant critical benchmark experiments.\n\n\n\nANS. 2017. “ANSI/ANS-8.24-2017: Validation of Neutron Transport Methods for Nuclear Criticality Safety Calculations.” Standard. ANS.\n\n\nBarnes, L, S King, and R Pevey. 2008. “Evaluation and Extension of the Experimental Basis for Licensing 5-10.” Report. University of Tennessee; ADAMS Accession No. ML18235A097.\n\n\nBess, John D, and Jim Gulliford. 2015. “The 2015 Edition of the ICSBEP Handbook.” Trans. Am. Nucl. Soc 113: 734–37.\n\n\nBess, John D, Tatiana Ivanova, Ian Hill, and Margaret A Marshall. 2018. “The 2018 Edition of the ICSBEP Handbook.” Trans. Am. Nucl. Soc 118: 523–26.\n\n\nBess, John D, Margaret A Marshall, Jim Gulliford, and Ian Hill. 2016. “The 2016 Edition of the ICSBEP Handbook.” Trans. Am. Nucl. Soc 115: 917–20.\n\n\nBroadhead, B. L., B. T. Rearden, C. M. Hopper, J. J. Wagschal, and C. V. Parks. 2004. “Sensitivity-and Uncertainty-Based Criticality Safety Validation Techniques.” Nuclear Science and Engineering 146: 340–66.\n\n\n———. 2004. “Sensitivity-and Uncertainty-Based Criticality Safety Validation Techniques.” Nuclear Science and Engineering 146: 340–66.\n\n\nBrown, David A, MB Chadwick, R Capote, AC Kahler, A Trkov, MW Herman, AA Sonzogni, et al. 2018. “ENDF/b-VIII. 0: The 8 Th Major Release of the Nuclear Reaction Data Library with CIELO-Project Cross Sections, New Standards and Thermal Scattering Data.” Nuclear Data Sheets 148: 1–142.\n\n\nDean, JC, and RW Tayloe. 2001. “Guide for Validation of Nuclear Criticality Safety Calculational Methodology.” Report. ORNL. https://www.nrc.gov/docs/ML0502/ML050250061.pdf.\n\n\nEckleberry, Troy, and Ivan G Maldonado. 2016. “Reactivity Impact of Accident Tolerant Claddings in an Equilibrium PWR Core.” Trans. Am. Nucl. Soc 115: 1195–98.\n\n\nGeorge, Nathan Michael, Ivan Maldonado, Kurt Terrani, Andrew Godfrey, Jess Gehin, and Jeff Powers. 2014. “Neutronics Studies of Uranium-Bearing Fully Ceramic Microencapsulated Fuel for Pressurized Water Reactors.” Nuclear Technology 188 (3): 238–51.\n\n\nMarshall, William BJ, Mark L Williams, Dorothea Wiarda, Bradley T Rearden, Michael E Dunn, Don Mueller, Justin B Clarity, and Elizabeth L Jones. 2015. “Development and Testing of Neutron Cross Section Covariance Data for SCALE 6.2.” Oak Ridge National Lab.(ORNL), Oak Ridge, TN (United States).\n\n\nNRC. 2014. “Spent Fuel Storage or Transportation System Misloading.” Report. U.S. NRC. http://adamswebsearch2.nrc.gov/webSearch2/view?AccessionNumber=ML14121A469.\n\n\n———. 2017. “Project Plan for Regulating Accident Tolerant Fuel.” Report. NRC. https://www.nrc.gov/docs/ML1727/ML17279A203.html.\n\n\nORNL. 2009. “SCALE: A Comprehensive Modeling and Simulation Suite for Nuclear Safety Analysis and Design, Version 6.0.” Computer Program. ORNL.\n\n\n———. 2009. “SCALE: A Comprehensive Modeling and Simulation Suite for Nuclear Safety Analysis and Design, Version 6.0.” Computer Program. ORNL.\n\n\nPerfetti, CM, and BT Rearden. 2018. “Ensuring the Fidelity of Data Assimilation Methodology Bias Estimates.” Trans. Am. Nucl. Soc 118: 567–70.\n\n\nPerfetti, CM, BT Rearden, and WJ Marshall. 2018. “Estimating Computational Biases for Criticality Safety Applications with Few Neutronically Similar Benchmarks.” Trans. Am. Nucl. Soc 118: 561–64.\n\n\nRearden, BT, and MA Jessee. 2016. “SCALE Code System, ORNL/TM-2005/39, Version 6.2.” Computer Code. Oak Ridge National Laboratory, Oak Ridge, Tennessee.\n\n\nScaglione, JM, DE Mueller, JC Wagner, and WJ Marshall. 2012. “An Approach for Validating Actinide and Fission Product Burnup Credit Criticality Safety Analyses–Criticality (k Eff ) Predictions.” Report. ORNL. https://www.nrc.gov/docs/ML1211/ML12116A128.pdf.\n\n\nShapiro, Rachel A, Ian M Younker, and Massimiliano Fratoni. 2013. “Neutronic Performance of Accident Tolerant Fuels.” Trans. Am. Nucl. Soc 109: 1351–53.\n\n\nSmith, K., S. Tarves, T. Bahadir, and R. Ferrer. 2017. “EPRI Report 1022909, ’Benchmarks for Quantifying Fuel Reactivity Depletion Uncertainty,’ Revision 1.” Report. Electric Power Research Institute, Palo Alto, CA; ADAMS Accession No. ML18088B397.\n\n\nSobes, Vladimir, Bradley T Rearden, Don Mueller, William BJ Marshall, John M Scaglione, and Michael E Dunn. 2015. “Upper Subcritical Limit Calculations with Correlated Integral Experiments.” Trans. Am. Nucl. Soc 112: 467–70.\n\n\nModeled for both normal and contingency cases with varying fuel enrichments of 5%, 6%, 8% and 10%. In addition, late in the project a “damp powder” base case was identified and a preliminary analysis of this case was added to the scope.↩︎\nGenerated by the SCALE/TSUNAMI-IP sequence.↩︎\nWhile arbitrary, a multiple misloaded fresh fuel assembly in a region with degraded fixed NAM is a credible accident (NRC 2014). Furthermore, to mitigate this accident, soluble boron is likely to be required.↩︎\nLEU-COMP-THERM-096: Partially-reflected water-moderated square-pitched U(6.90)O2 fuel rod lattices with 0.67 fuel to water volume ratio (0.800 cm pitch) added in 2015 (Bess and Gulliford 2015); LEU-COMP-THERM-097: Titanium and/or aluminum rod-replacement experiments in fully-reflected water-moderated square-pitched U(6.90)O2 fuel rod lattices with 0.67 fuel to water volume ratio (0.800 cm pitch) added in 2016 (Bess et al. 2016). LEU-COMP-THERM-098: Moderator-controlled critical experiments with UO2-5.74 wt% U-235 fuel rods added in 2018 (Bess et al. 2018).↩︎\nAll from LEU-COMP-THERM-008, LEU-COMP-THERM-047, LEU-COMP-THERM-051, LEU-COMP-THERM-055, and LEU-COMP-THERM-076.↩︎\nAll from LEU-COMP-THERM-008, LEU-COMP-THERM-011, LEU-COMP-THERM-014, LEU-COMP-THERM-028, LEU-COMP-THERM-047, LEU-COMP-THERM-051, LEU-COMP-THERM-076, LEU-COMP-THERM-081, and HEU-SOL-THERM-044. Given that the HEU-SOL-THERM-044-011 experiment, with a ck of 0.8605, had the highest ck among any of the 56 experiments in the validation set, it is not clear what the implications of this observation are as this experiment contains highly enriched uranyl nitrate (U-235 wt% of 93.172) solution – much greater than the 20 wt% U-235 in the application. That is, what would be the arguments for not allowing this experiment to be included in the validation set given the greater enrichment and the extraneous materials such as nitrogen? It would seem that as long as the most important reactions – those related to U-235 in this case – are covered by the experiment, and the sensitivity (and/or corresponding nuclear data uncertainty) of extraneous materials in the questionable experiment have a negligible impact, then the experiment should be allowed in the validation set.↩︎\nNew covariance data based on ENDF/B-VIII.0 is already available for consideration in a future SCALE release (Brown et al. 2018).↩︎\nAlternative, non-parametric methods are discussed in NUREG/CR-6698 (Dean and Tayloe 2001). It’s also worth mentioning that the correlation between experiments needs to be considered when performing a regression analysis (Sobes et al. 2015). For example, if the majority of the experiments come from the same experimental series, this data dependency (i.e., lack of independence) causes a certain amount of information loss that could significantly increase the bias uncertainty.↩︎\nUse of TSURFER for criticality code validation for burnup credit applications has also been previously discussed in NUREG/CR-7109.↩︎\nAssuming a sufficient number of experiments exists with ck greater than or equal to 0.80. The single sided lower tolerance limit (among others), documented in NUREG/CR-6698, can be used, and can also be determined automatically by SCALE/USLSTATS via TSUNAMI-IP.↩︎\nTwo to three times the total relative standard deviation of keff due to uncovered sensitivity data could be used.↩︎\n",
    "preview": "papers/2018-10-01-atf-sfp-csa-validation/atf-sfp-csa-validation_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-01-30T15:49:55+05:45",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "papers/2016-06-09-depletion-uncertainty-meta-analysis/",
    "title": "Comparison of Pressurized Water Reactor Depletion Code Validation Approaches",
    "description": "",
    "author": [
      {
        "name": "Amrit Patel",
        "url": {}
      }
    ],
    "date": "2016-06-09",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nKopp 5%\nMonte Carlo Uncertainty Sampling\nDirect Difference\nElectric Power Research Institute Reactivity Benchmarks\n\nAnalysis\nData Sources\nModeling\nComparison of Independent 95/95 Depletion Code Bias Plus Uncertainty Characterizations\n\nConclusion\nAppendix A: Determination of 95/50 Tolerance Interval Penalty Factor to Apply to EPRI-Calculated Regression Uncertainty\n\nIntroduction\nIn this analysis, 5 depletion code validation approaches are compared side-by-side. Each approach is briefly described, the statistical approaches used in this analysis are explained, and finally the resultant depletion uncertainty versus burnup characterizations are compared visually and discussed. Finally, some conclusions are made with implications for future spent fuel pool (SFP) criticality safety analyses (CSAs) that will be submitted to the NRC for review and approval.\nKopp 5%\nSince approximately 1998, NRC commercial power plant licensees have adopted the “Kopp 5%” approach in lieu of a formal depletion code validation. This has been documented in several recently issued SFP CSA safety evaluation reports and is explained further in NRC internal guidance document DSS-ISG-2010-01. The results of this approach are practically regarded as a “depletion uncertainty” in SFP CSAs submitted to the NRC to satisfy 10 CFR 50.68(b)(4) requirements. Consequently, for simplicity, the uncertainty arising from (or attributed to) the use of depletion codes to predict spent fuel isotopics, in terms of reactivity, will be reffered to as “depletion uncertainty” for the remainder of this report.\nFundamentally, the Kopp 5% approach is based on reactor design code uncertainties (e.g. lattice physics depletion codes – or depletion codes – in conjunction with reactor simulator codes). All NRC power plant licensees are required to use only reactor design codes that have been specifically approved for use by the NRC at the respective licensee facility. It has been observed that implementation of these codes for reactor core and cycle design generally result in various relevant figures of merit well within 5% of measured values irrespective of reactor core design code type and licensee.\nConsequently, most commercially licensed light water reactor licensees use the Kopp 5% approach since they already use previously licensed reactor design codes to perform depletion calculations in support of other design basis calculations (e.g. core design and transient analyses).\nThe major pitfall with this method is that it is not a generally applicable method and it is ultimately based on the observation that depletion codes in conjunction with reactor simulator codes can generally predict certain measured values within 5%. Because these predictions are generally within 5% of measured values by a large margin, this approach has been considered to be conservative. However, the relationship between the isotopic prediction accuracy of the depletion and simulator codes has not been formally tied to the Kopp 5% approach in terms of reactivity. Rather, it is implied, for example, that a relative assembly power prediction accuracy bounded by 5% directly translates into SFP reactivity prediction accuracy within 5%. The implication that this is true is the engineering judgment being made. The main goal of this study is to attempt to qualify this engineering judgment based on comparison to formal validation approaches.\nMonte Carlo Uncertainty Sampling\nThe Monte Carlo uncertainty sampling method (MC USM) is described in detail in NUREG/CR-6811, “Strategies for Application of Isotopic Uncertainties in Burnup Credit” (Gauld 2003) and is implemented for a typical SFP storage configuration in NUREG/CR-7108, “An Approach for Validating Actinide and Fission Product Burnup Credit Criticality Safety Analyses - Isotopic Composition Predictions” (Radulescu et al. 2012). The intent of the NUREG/CR-7108 research effort was to enable the creation of guidance that would allow NRC licensees to perform formal, technically-justified, depletion code validation analyses.\nNUREG/CR-7108 states, “[The MC USM] is used to represent the effects of nuclide concentration uncertainty on k-effective values by sampling isotopic concentrations from uncertainty distributions developed from experimental data.” Essentially, distributions of measured-to-calculated (M/C) isotopic concentrations are formed for 28 nuclides1 – these are important major and minor actinides and fission products. The measurements are derived from radiochemical assays (RCA) and samples are available at various burnups between 7 and 61 gigawatt-days per metric ton uranium (GWd/MTU). The RCA data is split up into a total of three burnup bins that have been defined using trending analysis. Burnup-dependent M/C distributions are formed for each of the major and minor actinides while the fission product data is lumped into a single bin over the range of RCA data burnups.\nAs part of the MC USM approach, depletion calculations are performed, to the various burnups of interest, using the TRITON two-dimensional (2-D) depletion code with nuclear cross-section data from Version 7.0 of the Evaluated Nuclear Data File maintained by Brookhaven National Laboratory (ENDF/B-VII.0); TRITON is part of the SCALE, Version 6.1 computer code package (SCALE 6.1) (ORNL 2011).2 Isotopics from these depletion calculations are used in subsequent criticality calculations and these criticality calculations serve as the reference calculations that determine the burnup-dependent reference k-effective values for the analyzed SFP configuration.\nBetween 250-500 criticality calculations are then performed for each chosen burnup point used in establishing the depletion uncertainty versus burnup relationship. Each criticality calculation samples all 28 of the pre-defined burnup-dependent M/C distributions. NUREG/CR-7108 uses 7-9 burnup points to establish the depletion uncertainty versus burnup relationship, thus somewhere between 1750 and 4500 criticality calculations are needed with the MC USM approach.\nEstimates of bias can then be formed based on calculating the difference between the reference k-effective calculations (i.e. without using any sampled M/C values to replace as-calculated values) and the simple average of the 250-500 criticality calculation k-effective values based on sampling the M/C distributions. Application of a statistical tolerance factor then allows for determination of uncertainty at a 95% probability, 95% confidence (95/95) level. More specifically, this is the uncertainty that would cover 95% of all k-effective values possible for the given configuration with 95% confidence based on the data available.\nIt was seen that a positive bias was determined over all burnup indicating that the SCALE 6.1 TRITON depletion code used in the analysis tends to overpredict spent fuel reactivity; these positive biases are not typically credited in SFP CSAs. Consequently, only the bias uncertainty remains and is characterized as the 95/95 depletion uncertainty.\nThe major pitfall with this method is that the experimental procedure is associated with relatively high RCA measurement uncertainties and operational history uncertainties that likely compound with increasing burnup.3 These high experimental procedure uncertainties manifest as statistical noise. This noise is separate from the true variance that is sought, which is associated solely with the depletion code. Consequently, estimates of 95/95 depletion uncertainty from the MC USM will be conservatively inflated.\nDirect Difference\nThe direct difference approach, also described in detail in NUREG/CR-6811, was also implemented in NUREG/CR-7108 and makes use of the same raw measurement data that the MC USM uses. However, the direct difference approach is less rigorous compared to the MC USM and is less computationally expensive as it does not require thousands of criticality calculations, but only a hundred or so.\nThe direct difference approach does not require the formation of M/C distributions because no sampling is done. Instead, RCA measurements are used directly in criticality models. That is, a vector of 28 nuclides (plus oxygen) corresponding to a RCA measurement from a single position within an assembly at a given burnup are used to represent the fuel composition for a given criticality model. Separate cases are created for each RCA datum corresponding to a single burnup measurement and nuclide vector from which a k-effective is calculated using the same criticality code which is part of the overall k-effective validation procedure. The k-effective values from these calculations are then differenced with reference k-effective values from respective criticality calculations where measurements were not used in place of the as-calculated values. A fit of the direct difference data is then determined as a function of burnup – this can be used to determine the depletion code prediction bias as a function of burnup. A statistical tolerance interval for regression can then be calculated to determine an appropriate bias uncertainty. It should be noted that the bias, and more particularly, the bias uncertainty is very sensitive to the data fitting procedure and the statistical tolerance interval generation procedure, therefore care must be taken and an appropriate technical rationale must be given. One common tolerance interval for regression is the prediction interval. This is a 95/50 tolerance interval and allows for the interpretation that one is 50% confident that at least 95% of the data is within the interval. It should be noted that the SFP criticality regulatory requirement in 10 CFR 50.68(b)(4) requires a 95/95 tolerance interval.\nAs with the MC USM, the direct difference method shows a positive bias over all burnup.\nAn illustrative example is provided in NUREG/CR-7108 implementing the direct difference approach for an unpoisoned SFP storage rack application and concludes that:\n\n[The comparison of the Monte Carlo uncertainty sampling approach with the direct difference approach] provides some limited evidence that the two different methods can produce similar results, thereby providing limited reassurance in the Monte Carlo approach for uncertainty propagation. However, further work with the direct-difference method, including additional comparisons and fully addressing the considerations described below, is needed prior to drawing firm conclusions.\n\nThe major pitfall with this method is the same as that for the MC USM; that is, the relatively large experimental procedure uncertainties.\nElectric Power Research Institute Reactivity Benchmarks\nTwo reports were created by the Electric Power Research Institute (EPRI) detailing methods for validating pressurized water reactor (PWR) criticality calculations that credit depleted fuel in SFP storage configurations. EPRI Report 1022909, “Benchmarks for Quantifying Fuel Reactivity Depletion Uncertainty” (i.e. the “EPRI benchmark report”), details the use of flux map data to infer the uncertainty associated with depletion reactivity calculations using Studsvik Scandpower’s CASMO and SIMULATE-3 reactor analysis tools (Smith et al. 2011). EPRI Report 1025203, “Utilization of the EPRI Depletion Benchmarks for Burnup Credit Validation” (i.e. the “EPRI utilization report”), relates to the benchmark report by providing 11 calculational PWR depletion benchmarks allowing for determination of an application-specific depletion reactivity bias adjustment (Lancaster 2012).\nEPRI’s total uncertainty estimate does not include a 95/95-based regression fit uncertainty in their analysis. Instead, EPRI qualitatively discounts high variance data by explaining that the high variance is caused by “measurement uncertainty” not associated with true depletion code uncertainty. The NRC acknowledges that this could be true, but formal quantitative justification for this claim must be provided.4 Consequently, the depletion uncertainty is much smaller than any of the other characterizations.\nIt is interesting to note that, in Section 5 of the EPRI utilization report, an example validation analysis is presented based on SCALE6.1/TRITON5 (using ENDF/B-VII data) as the depletion code. Table 5-2, “Bias for the Reactivity Decrement with 100-Hour Cooling Using SCALE 6.1 and the ENDF/B-VII Cross-section Library,” shows the reactivity differences between SCALE6.1/TRITON and CASMO-5 predictions for a matrix of 66 cases (varying burnups between 10 and 60 GWd/MTU). The maximum bias is seen to be 260 percent millirho (pcm) indicating excellent agreement between SCALE6.1/TRITON and CASMO-5 depletion codes for a range of cases and fuel characteristics. This excellent agreement is useful to keep in mind when comparing the EPRI benchmarking effort for the SCALE 6.1 TRITON depletion code with the two NUREG/CR-7108 benchmarking efforts also performed with the SCALE 6.1 TRITON depletion code. That is, it is expected that performing a MC USM or direct difference approach to validate SCALE 6.1 TRITON will result in very similar depletion uncertainty versus burnup characterization when using the EPRI validation approach with either CASMO-5 or SCALE 6.1 TRITON if measurement uncertainties are small relative to depletion code uncertainty or if measurement uncertainties are similar between the two validation approaches.\nLike with the NUREG/CR-7108 approaches, the major pitfall with this method is that the experimental procedure is associated with relatively high measurement uncertainties. In this case the measurement uncertainty is based on the chosen depletion uncertainty inferencing method as EPRI notes in the 9th bullet of Section 7.1, “Interpretation of Data,” of the EPRI benchmark report. Like with the two NUREG/CR-7108 approaches, these measurement uncertainties are confounded with the uncertainty that comes solely from the depletion code.\nAnalysis\nThe objective of the following analysis is to see whether or not the various depletion code validation methods produce similar 95/95-based characterizations of depletion uncertainty as a function of burnup for typical SFP storage configurations. For the Kopp 5%, MC USM, and the EPRI-conducted validation approach, “bounding” values of depletion uncertainty were determined without a regression analysis step because a regression analysis was not performed from which a regression fit uncertainty was characterized. It should be noted that if a regression anlaysis is being performed, a regression analysis step that includes a 95/95 characterization of uncertainty (or similar) must be used for compliance with 10 CFR 50.68(b)(4). Since the EPRI-conducted validation approach did not include a 95/95 characterization of uncertainty (or similar) as part of their data analysis, the EPRI validation approach was adjusted by the NRC staff to account for the regression analysis uncertainty for consistency with 10 CFR 50.68(b)(4). A 95/95-based regession analysis uncertainty was also characterized for the NUREG/CR-7108 direct difference validation approach since it also relies on regression analysis.\nData Sources\nThe direct difference data was taken directly from Table 6.3, “Unpoisoned PWR SFP storage rack \\(\\Delta k_{eff}\\) obtained with measured and calculated nuclide concentrations” in NUREG/CR-7108. The criticality models are based on a representative PWR SFP storage rack model crediting both actinides and fission products.\nThe MC USM data was taken directly from Table 7.2, “\\(k_{eff}\\) bias and \\(k_{eff}\\) bias uncertainty for the unpoisoned PWR SFP storage rack model using burnup credit actinide and fission product nuclides” in NUREG/CR-7108. As with the direct difference approach, the criticality models used in the MC USM approach are based on a representative PWR SFP storage rack model crediting both actinides and fission products.\nThe EPRI data in the analysis comes from: (1) the total uncertainty component derived in the EPRI benchmark report, which is summarized in Table 10-1, “Measured Cold Reactivity Decrement Bias and Uncertainty” and (2) the bias component derived in Section 6 of the EPRI utilization report applicable to the SCALE 6.1 depletion code used with ENDF/B-VII data. It should be noted that bias and uncertainty components are added together to form an effective 95/95 limit for consistent comparison to the other approaches that incidentally do not have bias components.\nThe Kopp 5% data in the analysis comes from an ongoing NRC effort to determine the expected reactivity impact due to specific SFP biases and uncertainties under various conditions. The NRC performed a review of approved license amendment requests (LARs) which include information on biases and uncertainties in SFP nuclear criticality safety (NCS) analyses. This document review has allowed the NRC to consolidate and assess a large amount of SFP NCS analysis data from NRC-approved real world applications. This data is relevant because (1) the SFP NCS data was produced through methodologies that have been reviewed and approved by the NRC, and (2) it represents practical application of NCS analysis methodologies, rather than theoretical situations established for the purposes of sensitivity studies.\nThe Kopp 5% data was collected from 33 approved SFP LARs dating back to the year 2000, which resulted in a database consisting of results from 163 individual criticality calculations. LARs that were withdrawn or LARs where the NRC did not explicitly review the NCS analysis were not included in the database.\nModeling\nThe R environment for statistical computing (R Core Team 2015) was used to determine approximate 95/95 tolerance intervals using both the R predict method for class lm with corrections using one-sided tolerance factors (based on the non-central t distribution) and two-sided tolerance factors using the formulations based on Section 7.2.6.3, “Tolerance intervals for a normal distribution,” of the National Institute of Standards and Technology Engineering Statistics Handbook (Natrella 2013).6\nPrediction interval correction in regression analyses\nIt is possible to define a regression tolerance interval that accounts for increasing dependent variable variance as a function of the independent variable (i.e. heteroskedasticity) (Myhre et al. 2010). However, this was not determined in Wallis’ 1951 work on the subject (Wallis 1951). The necessary modification of Wallis’ work to account for heteroskedasticity is not part of the R tolerance library used in this analysis, and is therefore beyond the scope of this analysis. Instead, a conservative simplification was used.\nFor both the direct difference and NRC-adjusted EPRI approaches, 95% two-sided prediction intervals were divided by 95/50 two-sided tolerance factors and multiplied by one-sided 95/95 tolerance factors to determine effective 95/95 tolerance intervals; for determining regression tolerance factors, this requires calculation of the “effective number of observations” (Young 2010).7 The reasoning behind using a 95/50 tolerance factor is based on the relationship between a prediction interval and a tolerance interval. That is, “a prediction interval can be stated to cover a specified proportion of a population on average, whereas a tolerance interval covers it with a certain confidence level” (Wikipedia 2016). Taking this at face value, this would imply that a prediction interval can be approximated by a 95/50 tolerance interval. Using the definition of the prediction interval for the case of unknown mean and unknown variance and equating this to the tolerance interval, it is assumed that:\n\\[ T_{(1-p/2)} * \\sqrt{1+(1/n)} \\approx \\sqrt{\\frac{(n-1) * (1+1/n) * z_{(1-p)/2}^2}{\\chi_{\\gamma,n-1}^2}} \\]\nwhere n is the sample size, \\(T_{(1-p/2)}\\) is the 100(1 - p/2)th percentile of Student’s t-distribution with n − 1 degrees of freedom, \\(z_{(1-p)/2}\\) is the critical value of the normal distribution associated with cumulative probability (1 - p)/2, and \\(\\chi_{\\gamma,n-1}^2\\) is the critical value of the \\(\\chi^2\\) distribution with n - 1 degrees of freedom with probability \\(\\gamma\\).\nAs a check to see if the approximation above is valid, the two-sided 95% “prediction factor” was calculated and compared to the two-sided 95/50 tolerance factor for various sample sizes that bound those of the direct difference and EPRI datasets.\n\n\nShow code\n\n# Set default font size to avoid setting for each plot individually\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(extrafont)\nlibrary(tolerance)\nscale_colour_discrete <- function(...) scale_colour_brewer(..., palette=\"Set1\")\nscale_fill_discrete <- function(...) scale_fill_brewer(... , palette=\"Set1\")\ntheme_set(theme_tufte(base_size = 10))\n\n# Function to calculate tolerance factors based on sample size for various confidence levels\nfactorCheck <- function(size) {\n    Confidence <- seq(from=0.1, to=0.9, by=0.1)\n    toleranceFactor <- c()\n    for (i in Confidence) {\n        tol <- K.factor(n=size, f=size-1, alpha=1-i,\n                            P=0.95, side=2, method=\"EXACT\", m=100)\n        toleranceFactor <- append(toleranceFactor, tol)\n    }\n    return(toleranceFactor)\n}\n\n# Assemble a dataframe containing selected cases for assumption verification \ncheck <- tbl_df(data.frame(Confidence=seq(from=0.1, to=0.9, by=0.1)))\ncases <- c(10, 100, 1000)\nfor (i in cases) {\n    check <- bind_cols(check, as.data.frame(factorCheck(i)))\n}\n\n# Format the data for plotting\ncolnames(check)[2:4] <- cases\ncases <- tbl_df(reshape2::melt(check, id.vars=\"Confidence\", variable.name=\"Case\", value.name=\"TolFactor\"))\ncases$Case <- as.numeric(levels(cases$Case))[cases$Case]\ncases <-\n    cases %>%\n    mutate(PredFactor=qt(p=((1+0.95)/2), df=Case-1) * sqrt(1+1/Case)) %>%\n    mutate(DeltaFactor=TolFactor - PredFactor)\ncases$Case <- as.factor(cases$Case)\n\nggplot(cases, aes(Confidence, DeltaFactor, color=Case)) +\n    geom_point() + \n    geom_line() +\n    geom_vline(xintercept=0.5) +\n    geom_hline(yintercept=0) +\n    labs(title=\"Verification of equating 95% prediction factor with 95/50 tolerance factor\")\n\n\n\n\nThe plot directly above shows the difference between the 95/50 tolerance factors and the 95% prediction factors. The plot provides confirmation that the assumption of a 95/50 tolerance interval being approximately the same as a 95% prediction interval is valid for any sample size. The fact that the confidence level of the equivalent tolerance factor is greater than 50% indicates that using the 95/50 tolerance factor to represent the 95% prediction factor is conservative.\nDirect difference data analysis approach\nFor the NUREG/CR-7108 direct difference data analysis, a weighted linear least-squares (WLS) regression model was selected due to the observed heteroskedasticity with burnup. A major assumption that goes into the WLS model is the choice of burnup-dependent weights, as this dictates both the regression line and the prediction interval(s). The weights were rigorously selected based on conditional variance function estimation as discussed in Section 7.3, “Conditional Variance Function Estimation” using an iterative process from the Advanced Data Analysis from an Elementary Point of View textbook; the regression line was also determined iteratively (Shalizi 2016).8 Specifically, the iterative.wls function in Chapter 7 was used without modification. The direct difference data residuals were fit non-parametrically as a function of burnup and this fit was used directly for the weights in the WLS regression model and more importantly in generating the corresponding burnup-dependent prediction interval. The reason for using this approach to define the regression line, weights, and prediction interval was to treat the data as objectively as possible without prescribing arbitrary constraints.\nA two-sided 95% prediction interval was used as the basis for the direct difference data regression fit uncertainty with a 95/95 tolerance factor adjustment. The 95/95 tolerance factor adjustment is dictated by the effective number of observations corresponding to the particular dataset. The effective number of observations is given by the following formulation:\n\\[ n_{i}^{*} = \\frac{\\hat{\\sigma}^2}{s.e.(\\hat{y_i})^2} \\]\nwhere \\(n_{i}^{*}\\) is the effective number of observations for the ith observation, \\(\\hat{\\sigma}\\) is the sample standard deviation, and \\(s.e.(\\hat{y_i})\\) is the standard error of the ith observation. The sample standard deviation is approximated by the root mean square error (RMSE), which, in this case, reflects the deviation of the predictions about a reference observed value of 0 for all burnups. The reference observed value of 0 represents the case of perfect agreement between code predictions and measurements, therefore the RMSE with this as a reference should give the desired measure of “standard deviation” required for the effective number of observations calculation. In this analysis, all of the data was used to calculate the RMSE.\nTo simplify the calculation of the standard error of the ith observation, a single conservative standard error was chosen to represent all observations. This was done by calculating the standard deviation of all observations greater than or equal to a burnup of 30 GWd/MTU where the variance of the data becomes significantly higher. Due to the pronounced heteroskedasticity of the data, it would be more accurate to define a burnup-dependent estimate of standard error; however, it was desirable to have a single effective number of observations for use in estimating a reasonably conservative 95/95 tolerance factor.\nNRC-adjusted EPRI data analysis approach\nFor the NRC-adjusted EPRI validation approach, EPRI’s regression model was used to characterize the bias. However, the upper limit of (the assumed to be) two-sided 95% prediction interval from Figure 9, “Reactivity Quadratic Regression for Cycle-collapsed Data” from Attachment 2, “EPRI Analysis for Determining the 95/95 Confidence Limits on CASMO Hot Full Power (HFP) Measured Reactivity Decrement Bias Regressions Used in the EPRI/Studsvik Burnup Benchmark” as part of the “Responses to Follow-up RAIs for EPRI Report 1022503 and EPRI Report 1022909” was used as the basis for the regression fit uncertainty (Cummings 2016). Note that because the heteroskedasticity of the EPRI data is not as pronounced as that for the direct difference data, the 95/95 tolerance factor determined is not as penalizing at lower burnups as in the direct difference case. The Appendix contains annotated R source code showing the generation of this approximate one-sided 95/95 tolerance interval and the various assumptions made.\nComparison of Independent 95/95 Depletion Code Bias Plus Uncertainty Characterizations\nThe figure directly below shows both the data (where available) along with the corresponding independent 95/95 depletion code bias and uncertainty characterizations. Note that the “EPRIstat” data corresponds to the NRC-adjusted EPRI validation approach. The overall shape of the bias plus uncertainty intervals are very similar among all of the approaches except for the EPRI characterization which is seen to be relatively constant and much smaller than the others as a function of burnup. It should also be noted that lower rather than upper intervals for all of the approaches are derived because the negative reactivity differences indicate depletion code underprediction of spent fuel reactivity and it is desirable to find the statistically limiting amount of underprediction as a function of burnup. This needs to be confirmed to be true for the EPRI data.\n\n\nShow code\n\n# Script to generate plot comparing depletion validation approaches\n\n# Load direct difference data\nNUREG7108 <- tbl_df(read.csv(\"data/7108DDdata.csv\")) %>% mutate(db=\"directDiff\") %>% select(db, burnup, dk)\n\n# Load SFP NCS database to pull Kopp 5% data\n#source(\"scripts/loadSFPncsData.R\")\nload(\"data/sfpCritUncAnalysis.Rdata\")\npwrDepUnc <- LWRtols %>% filter(db==\"NRC-PWR\", tol==\"depUnc\") %>% mutate(db=\"Kopp5%\") %>% select(db, burnup, dk) %>% na.omit()\n\n# Load EPRI data - this dataframe is constructed by adding the CASMO-5 95% uncertainty from Table 10-1 in EPRI Report 1022909, Rev. 0 (2011) with the 150 pcm bias determined from Section 6 of EPRI Report 1025203, Rev. 0 (2012).\nEPRIbias <- rep(-(150+(643-576))/1e5, 6)  # from utilization report for SCALE 6.1 w/ ENDF/B-VII data\nEPRIunc <- c(-0.00521, -0.00576, -0.00571, -0.00560, -0.00540, -0.00534)  # from benchmark report for CASMO-5 + SIMULATE-3 w/ ENDF/B-VII data + others\nEPRItot <- EPRIbias + EPRIunc\nEPRI95 <-\n    data.frame(db=c(\"EPRI\",\"EPRI\",\"EPRI\",\"EPRI\",\"EPRI\",\"EPRI\"),\n               burnup=c(10,20,30,40,50,60),\n               dk=EPRItot\n               )\n\n# Merge separate datasets into one for more convenient processing\n\npPWRdepU <- bind_rows(NUREG7108, pwrDepUnc, EPRI95) %>% group_by(db) %>% arrange(db, burnup)\n\n# Fit models from datasets\n\n# Fit the data to a linear model\n\n    # An ordinary linear least-squares fit is performed on all non-direct-difference data\n    #library(broom)\n    #library(tolerance)\n    linFits <-\n        pPWRdepU  %>% do(regtol.int(reg = lm(dk~burnup, .), new.x=NULL, alpha=0.05, P=0.95, side=1)) %>%\n        filter(db!=\"directDiff\")\n    \n    # A weighted linear least-squares fit is performed on direct-difference data\n    w <- NUREG7108$burnup^1.0 \n    linFitsWgtDD <-\n         pPWRdepU %>% filter(db == \"directDiff\") %>%\n         do(regtol.int(reg = lm(dk~burnup, weights = 1/w, .), new.x=NULL, alpha=0.05, P=0.95, side=1))\n\n# Merge the original data with the fit + tolerance interval\n\n    ### Process the LS fits for plotting\n    allLinFits <- bind_rows(linFits, linFitsWgtDD)\n    tols <- tbl_df(allLinFits) %>% select(everything(), lwr=contains(\"lower\"), upr=contains(\"upper\")) %>% arrange(db)\n    bu <- pPWRdepU %>% arrange(db) %>% group_by() %>% select(burnup)\n    dk <- pPWRdepU %>% arrange(db) %>% group_by() %>% select(dk)\n    pLinFits <- bind_cols(bu, dk, tols) %>% arrange(db, burnup) %>% select(db, everything())\n\n# Plot desired datasets    \n\n    # Plot of WLS direct difference data fit and other OLS data fits\n    # Translating Kopp 5% curve for comparison using consistent reactivity difference basis\n    fpLinFits <-\n        pLinFits %>%\n        mutate(dk2=ifelse(db==\"Kopp5%\", -1*dk, dk)) %>% select(-dk, dk=dk2) %>%\n        mutate(lwr2=ifelse(db==\"Kopp5%\", -1*lwr, lwr)) %>% select(-lwr, lwr=lwr2) %>%\n        mutate(upr2=ifelse(db==\"Kopp5%\", -1*upr, upr)) %>% select(-upr, upr=upr2) %>%\n        mutate(y.hat2=ifelse(db==\"Kopp5%\", -1*y.hat, y.hat)) %>% select(-y.hat, y.hat=y.hat2)\n\n    fpLinFits <-\n        fpLinFits %>% mutate(lwr=ifelse(db==\"Kopp5%\", upr, lwr)) %>% mutate(upr=ifelse(db==\"Kopp5%\", lwr, upr)) %>% rename(fit=y.hat)\n    \n    # Replace 95% prediction interval with 95/95 tolerance interval\n    \n    # Function used to define WLS weights\n    iterative.wls <- function(x, y, tol = 0.01, max.iter = 100) {\n        iteration <- 1\n        old.coefs <- NA\n        regression <- lm(y ~ x)\n        coefs <- coefficients(regression)\n        while (is.na(old.coefs) || ((max(coefs - old.coefs) > tol) && (iteration <\n        max.iter))) {\n        variance <- npreg(residuals(regression)^2 ~ x)\n        old.coefs <- coefs\n        iteration <- iteration + 1\n        regression <- lm(y ~ x, weights = 1/fitted(variance))\n        coefs <- coefficients(regression)\n        }\n        return(list(regression = regression, variance = variance, iterations = iteration))\n    }\n    \n    # Grab direct difference data\n    DDdata <- fpLinFits %>% filter(db==\"directDiff\") %>% select(burnup, dk)\n    library(np)\n    # Calculate the residuals of the direct difference data\n    log <-\n        capture.output({\n           test <- iterative.wls(DDdata$burnup, DDdata$dk)\n        })\n    varPoints <- residuals(test$variance)\n    # Fit the residual over burnup to get WLS weights\n    varFit <- predict(test$variance, exdat=DDdata$burnup)\n    \n    # This is to visually check the weight function\n    # plot(x=DDdata$burnup, y=varPoints)\n    # lines(DDdata$burnup, varFit)\n    \n    # This is to see how the direct difference delta-k data is distributed \n    # q <- DDdata$dk\n    # par(mfrow=c(1,2))\n    # hist(q, main=\"Distribution\", xlab=\"K-eff\")\n    # qqnorm(q)\n    # qqline(q)\n    # result <- shapiro.test(q)\n    # result$p.value\n\n    # Set the WLS weights equal to the fit of the direct difference residuals\n    w <- varFit\n    # Calculate the two-sided direct difference 95% prediction interval\n    linFitsWgtDD <- predict(test$regression, DDdata, interval = \"prediction\", weights = 1/w)\n    linFitsWgtDD <- tbl_df(as.data.frame(linFitsWgtDD))\n    \n    # Function definining the one-sided tolerance factor\n    k1 <-\n        function(samp, cov, conf){\n            n = samp                  # number of samples\n            p = cov                   # coverage parameter\n            g = conf                  # confidence parameter\n            f = n - 1                 # degrees of freedom\n            delta = qnorm(p)*sqrt(n)  # non-centrality parameter\n            k = qt(g,f,delta)/sqrt(n) # k-factor for a one-sided tolerance interval\n            return(k)\n        }\n    \n    # The RMSE is estimated to be:\n    library(hydroGOF)\n    RMSE <- rmse(sim=NUREG7108$dk, obs=rep(0, length(NUREG7108$dk)))\n    # The standard error is estimated to be:\n    subset <- NUREG7108 %>% filter(burnup >= 30) %>% select(dk)  # a subset of the data is used for conservatism\n    stdErr <- sd(subset$dk)/sqrt(length(subset$dk))\n    # Therfore, the effective number of observations is:\n    nEff <-  RMSE^2/stdErr^2\n    # To determine a penalty for using a 95/50 regression tolerance interval instead of a 95/95 regression tolerance interval, the regression fit uncertainty is increased by the following factor:\n    adjFactorDD <- k1(nEff,0.95,0.95)/K.factor(n=nEff, f=nEff-1, alpha=1-0.50, P=0.95, side=2, method=\"EXACT\", m=100)\n\n    # The original homoskedastic 95/95 tolerance interval is replaced with an effective 95/95 heteroskedastic tolerance interval\n    linFitsWgtDD <-\n        linFitsWgtDD %>%\n        bind_cols(data.frame(burnup=sort(NUREG7108$burnup)), .) %>%\n        ## This modeling assumption assumes the 95/95 tolerance factor correction corresponding to the \n        ## highest burnup also applies to all other burnups; this is likely overly conservative.\n        mutate(lwr=lwr*adjFactorDD) %>%\n        ## This modeling assumption discounts the 95/95 tolerance factor correction for lower burnups\n        ## based on the shape of the data; this is likely more reasonable, but uses engineering\n        ## judgment.\n        # mutate(shape=ifelse(adjFactorDD*lwr/min(lwr) > 1, adjFactorDD*lwr/min(lwr), 1) ) %>%\n        # mutate(lwr=lwr*shape) %>% \n        select(-upr)\n    \n    # This is to visually check the shape of the burnup-dependent correction\n    # ggplot(linFitsWgtDD %>%\n    #           melt(., id.vars=c(\"burnup\")) %>%\n    #           group_by(variable) %>%\n    #           mutate(normVal=value/last(value)) %>%\n    #           filter(variable==\"lwr\" | variable==\"shape\") %>%\n    #           select(-value),\n    #       aes(burnup, normVal, color=variable)) + geom_line()\n    \n    # Add the effective 95/95 heterodskedastic tolerance interval to the dataframe with the other approaches\n    fpLinFits <- fpLinFits %>%\n                    mutate(lwr=ifelse(db==\"directDiff\", linFitsWgtDD$lwr, lwr)) %>%\n                    # mutate(upr=ifelse(db==\"directDiff\", linFitsWgtDD$upr, upr)) %>%\n                    mutate(fit=ifelse(db==\"directDiff\", linFitsWgtDD$fit, fit)) %>%\n                    select(-upr)\n    \n    # Perform the NRC-adjusted EPRI approach\n    source(\"scripts/k1correctionEPRIstat.R\") \n    EPRIuncData <- EPRIadj\n    \n    # Create and format the EPRIstat dataframe to be compatible with combined dataframe\n    EPRIstat <-\n        fpLinFits %>%\n        filter(db==\"EPRI\") %>%\n        mutate(dk=rep(NA, 6)) %>%\n        mutate(fit=c(19, 46, 81, 125, 177, 238)/1e5) %>%\n        mutate(lwr=-EPRIuncData$BandU/1e5) %>%\n        mutate(db=\"EPRIstat\")\n    \n    # Add the EPRIstat dataframe to the combined dataframe\n    fpLinFits <- \n        bind_rows(fpLinFits, EPRIstat) %>% arrange(db)\n\n    # Update and format the EPRI dataframe to be compatible with combined dataframe\n    EPRIupd <-\n        fpLinFits %>% \n        filter(db==\"EPRI\") %>%\n        mutate(dk=rep(NA, 6)) %>%\n        mutate(fit=c(19, 46, 81, 125, 177, 238)/1e5) %>%\n        mutate(lwr=EPRItot)\n        \n    # Update the EPRI data in the combined dataframe\n    fpLinFits <- bind_rows(fpLinFits %>% filter(db!=\"EPRI\"), EPRIupd) %>% arrange(db)\n    \n    # Create and format the MC USM dataframe to be compatible with combined dataframe\n    MCUSM <- data.frame(burnup=c(25,40,50,60), dk=c(0.0190,0.0188,0.0219,0.0300), fit=c(0.0046,0.0034,0.0061,0.0105))\n    MCUSM <- \n        tbl_df(MCUSM) %>%\n        mutate(db=\"MC USM\") %>% \n        mutate(alpha=NA, P=NA, y=NA, lwr=-dk, dk=NA) %>%\n        select(db, burnup, alpha, P, y, dk, lwr, fit) %>%\n        arrange(db)\n    \n    # Add the MC USM dataframe to the combined dataframe\n    fpLinFits <- bind_rows(fpLinFits, MCUSM)\n    \n    # Plot the actual bias and uncertainty characterizations with the underlying data where available\n    ggplot(fpLinFits,\n        aes(burnup, dk, color=db)) +  geom_point() + geom_line(aes(x=burnup, y=fit)) +\n        geom_ribbon(aes(ymin=lwr, ymax=fit), alpha=0.1) +\n        # scale_y_continuous(breaks = seq(-0.03, 0.05, by = 0.005), limits=c(-0.03, 0.05)) +\n        labs(title=\"Independent 95/95 depletion code bias and uncertainty characterizations\",\n             x=\"Burnup (GWd/MTU)\",\n             y=expression(paste(Delta, k)))\n\n\n\n\nThe linear fits seem to provide a good representation of the average behavior with the fit intercept tending toward 0 at 0 burnup as is physically expected for all of the validation approaches. However, one of the main observations is that due to the 95/95 aspect of the analyses, data outliers have disproportionate weight, and drive the overall uncertainty as is seen with the Kopp 5% and direct difference intervals.9\nAnother observation is that the MC USM and direct difference approaches are in relative agreement; however, the direct difference approach fails to pick up the more refined MC USM uncertainty shape. The direct difference tolerance interval also begins to decrease from a burnup of 54 GWd/MTU and up, which is a result of the non-parametric fitting of the direct difference data residuals discussed previously. While this is arguably unphysical, this is the result of an objective treatment of the available data.\nInterestingly, the NRC-adjusted EPRI approach uncertainty appears to agree relatively well with the two NUREG/CR-7108 approach characterizations confirming the possibility discussed earlier that performing a MC USM or direct difference approach to validate SCALE 6.1 TRITON could result in very similar depletion uncertainty versus burnup characterization when using the EPRI validation approach with either CASMO-5 or SCALE 6.1 TRITON. However, it should be emphasized that this may be purely coincidental since there were no attempts to quantify measurement uncertainties for either validation approach.\nThe formal 95/95 depletion code bias plus uncertainty characterizations also indicate that the Kopp 5% validation approach is not necessarily conservative.\nAs seen in the figure above, there is a positive bias that increases with increasing burnup for each of the approaches that make use of statistical tolerance intervals. This indicates that SFP reactivity is being overpredicted on average. Not crediting this positive bias is likely a significant conservatism. Note that by definition, the Kopp 5% only has an uncertainty component, therefore there is no bias component.\nFor the final bias treatment, no credit is taken for positive biases; negative bias should always be credited. For the final uncertainty treatment, the 95/95 uncertainty is defined by the difference between the lower 95/95 limit and the fit unless the fit is positive. If the fit is positive, then the difference between the lower 95/95 limit and zero is used to define the 95/95 uncertainty. This is done to reasonably discount any k-effective overprediction component from the uncertainty while not allowing it to be credited as a non-conservative bias; this is consistent with ANSI/ANS-8.27-2015, “Burnup Credit for LWR Fuel” (ANS 2015).10 The figure below shows the resultant 95/95 bias plus uncertainty characterizations compared to the OLS fit11 of the Kopp 5% data based on the preceding rules.\n\n\nShow code\n\n# Add negative bias and uncertainty components together\ncomp <-\n    fpLinFits %>%\n    group_by(db, burnup) %>%\n    arrange(db, burnup) %>%\n    # If Kopp 5% data, only plot the fit; otherwise, add negative bias and uncertainty components together subtracting off any positive bias component greater than 0 from the uncertainty component and not crediting any positive bias component greater than 0.\n    mutate(dk=ifelse(db==\"Kopp5%\", fit, ifelse(fit >= 0, lwr, fit+lwr))) %>%\n    select(db, burnup, fit, dk)\n    \n# Plot the bias plus uncertainty characterizations\nggplot(comp, aes(x=burnup, y=abs(dk), color=db)) + geom_line() +\n    # scale_y_continuous(breaks = seq(0, 0.03, by = 0.0025), limits=c(0, 0.03)) +\n    labs(title=\"Independent 95/95 depletion code bias plus uncertainty characterizations\",\n         x=\"Burnup (GWd/MTU)\",\n         y=expression(paste(\"abs(\", Delta, \"k)\")))\n\n\n\n\nConclusion\nThe SFP regulatory requirements of 10 CFR 50.68(b)(4) imply that 95/95 tolerance limits be calculated for any regression analysis used to support the validation of a depletion code for crediting burnup in SFP criticality calculations. The regression analyses performed and assumptions made in applying: (1) the direct difference approach to the NUREG/CR-7108 SFP example and (2) the EPRI depletion benchmark approach to the example using SCALE 6.1 with ENDF/B-VII data presents the NRC staff’s understanding of how the 10 CFR 50.68(b)(4) requirements can be satisfied with respect to formal depletion code validation approaches that rely on regression analysis.\nThe MC USM approach does not require regression analysis, and is not subject to the many assumptions required as part of regression analyses, therefore the MC USM approach can be considered to provide more reliable estimates of bias and bias uncertainty compared to the direct difference approach.\nThe Kopp 5% approach and EPRI’s implementation of the EPRI depletion benchmark approach are ultimately qualitative methods based on engineering judgment and the analyses performed in this report do not necessarily support use or continued use of these methods. The NRC staff recommends use of one of the formal depletion code validation approaches discussed in this report for use in future licensing applications.\nAppendix A: Determination of 95/50 Tolerance Interval Penalty Factor to Apply to EPRI-Calculated Regression Uncertainty\nTo determine a penalty for using a 95/50 regression tolerance interval (more commonly known as a prediction interval) instead of a 95/95 regression tolerance interval, the regression fit uncertainty is increased by an adjustment factor. In this analysis, this factor is based on dividing the regression fit uncertainty by the two-sided tolerance factor that covers 95% of the population 50% of the time and multiplying it by the one-sided tolerance factor that covers 95% of the populations 95% of the time.\nFirst, a function for calculating the one-sided tolerance factor based on the non-central t distribution is defined for use.12\n\n\nk1 <-\nfunction(samp, cov, conf){\n    n = samp                  # number of samples\n    p = cov                   # coverage parameter\n    g = conf                  # confidence parameter\n    f = n - 1                 # degrees of freedom\n    delta = qnorm(p)*sqrt(n)  # non-centrality parameter\n    k = qt(g,f,delta)/sqrt(n) # k-factor for a one-sided tolerance interval\n    return(k)\n}\n\n\n\nFor regression problems, regression tolerance intervals defined by use of regression tolerance factors are applicable. For regression tolerance factors, an “effective number of observations” must be determined. The effective number of observations can be calculated from the RMSE and the standard error of a given dataset.13\nThe higher burnup data was used to estimate the regression tolerance adjustment factor based on using a 95/50 regression tolerance interval rather than a 95/95 regression tolerance interval. This is for simplicity and conservatism; note that the data is slightly but not much more spread out at these higher burnups when compared to the data at lower burnups.\nThe higher burnup data was taken to be greater than 45 GWd/MTU and was estimated from Figure 9, “Reactivity Quadratic Regression for Cycle-collapsed Data” from Attachment 2, “EPRI Analysis for Determining the 95/95 Confidence Limits on CASMO Hot Full Power (HFP) Measured Reactivity Decrement Bias Regressions Used in the EPRI/Studsvik Burnup Benchmark” as part of the “Responses to Follow-up RAIs for EPRI Report 1022503 and EPRI Report 1022909” as follows:\n\n\nShow code\n\nhighBUdata <- c(750, 575, 550, 510, 500, 400, 375, 300, 250, 250, 250, 220,\n                210, 125, 115, -115, -115, -500)\nhighBUdata\n\n\n [1]  750  575  550  510  500  400  375  300  250  250  250  220  210\n[14]  125  115 -115 -115 -500\n\nThe effective number of observations is calculated by dividing the RMSE of the data by the standard error of the data as follows:\n\n\n# The root mean square error is estimated to be:\n\nRMSE <- rmse(sim=highBUdata, obs=rep(0, length(highBUdata))) \n# Note: 0 is used as the reference.\nRMSE\n\n\n[1] 385.98\n\n# The standard error is estimated to be:\n\nstdErr <- sd(highBUdata)/sqrt(length(highBUdata))\nstdErr\n\n\n[1] 69.55508\n\n# Therefore, the effective number of observations is:\n\nnEff <-  RMSE^2/stdErr^2\nnEff\n\n\n[1] 30.79441\n\nTo determine the penalty for using a 95/50 regression tolerance interval instead of a 95/95 regression tolerance interval, the regression fit uncertainty is then increased by an adjustment factor. This factor is based on dividing the regression fit uncertainty by the two-sided tolerance factor that covers 95% of the population 50% of the time and multiplying it by the one-sided tolerance factor that covers 95% of the population 95% of the time. Note that it was assumed that EPRI calculated a two-sided prediction interval rather than two one-sided intervals. The calculation was performed as follows:\n\n\nadjFactor <- k1(nEff,0.95,0.95) / K.factor(\n                                    n=nEff, \n                                    f=nEff-1, \n                                    alpha=1-0.50,\n                                    P=0.95,\n                                    side=2,\n                                    method=\"EXACT\", m=100\n                                  )\n\nadjFactor\n\n\n[1] 1.098272\n\nEPRI has previously calculated other uncertainty data that factors into the final 95/95 depletion uncertainty determination. This data, taken from the EPRI benchmark report, is as follows:\n\n\nShow code\n\nEPRIuncData <- tbl_df(read.csv(\"data/EPRIuncData.csv\"))\nknitr::kable(EPRIuncData)\n\n\nburnup\nHtoC\nfTemp\nregUncLow\nregUncHigh\n10\n380\n255\n250\n333\n20\n452\n255\n500\n667\n30\n446\n255\n750\n1000\n40\n430\n255\n800\n1167\n50\n410\n255\n900\n1300\n60\n398\n255\n1000\n1400\n\nThe burnup column is the burnup in GWd/MTU, “HtoC” is the 2-sigma hot-to-cold cross-section uncertainty, fTemp is the 2-sigma fuel temperature uncertainty as applied to the cold SFP environment, and regUncLow and regUncHigh are lower and upper prediction intervals, respectively, as estimated from Figure 9 of Attachment 2; the 95/50 penalty factor applies only to this regression uncertainty. Note that the upper prediction interval is being adjusted as it is larger than the lower prediction interval producing a more conservative uncertainty estimate. This was done because EPRI does not indicate whether positive reactivity decrement error means depletion code under- or over-prediction. Therefore, the most appropriate prediction interval could not be determined without additional information. The prediction interval corresponding to depletion code under-prediction should be used.\nTechnically, whether the depletion code under- or over-predicts on average is also important to determine whether or not a depletion code bias should be applied based on the regression analysis. EPRI notes that “the maximum width of the 95% confidence interval in Figure 9 [of Attachment 2] is 174 pcm at the high burnup end of the regression.” This would be interpreted to mean that at the high burnup end of the regression curve, that the depletion code could be in error by up to 174 pcm on average. If this corresponds to an average underprediction of 174 pcm, it is generally expected that this bias be accounted for; alternatively, a burnup-dependent bias would be justified. At the same time, it would also be reasonable to discount this underprediction from the width of the 95/95 tolerance interval when determining the 95/95 regression uncertainty; likewise, it would also be reasonable to discount any overprediction from the width of the 95/95 tolerance interval while not crediting the overprediction bias. This was also the approach taken when analyzing the NUREG/CR-7108 direct difference data in this report. For the purposes of the NRC-adjusted EPRI validation approach, no bias was accounted for, which is estimated to have a relatively small but not insignificant effect.\nThe adjusted uncertainty then becomes:\n\n\nShow code\n\nEPRIuncDataAdj <- EPRIuncData %>%\n    mutate(regUnc=round(regUncHigh * adjFactor))\nknitr::kable(EPRIuncDataAdj)\n\n\nburnup\nHtoC\nfTemp\nregUncLow\nregUncHigh\nregUnc\n10\n380\n255\n250\n333\n366\n20\n452\n255\n500\n667\n733\n30\n446\n255\n750\n1000\n1098\n40\n430\n255\n800\n1167\n1282\n50\n410\n255\n900\n1300\n1428\n60\n398\n255\n1000\n1400\n1538\n\nCombining the uncertainties by the root sum of squares (RSS) and adding depletion code specific biases in the SFP environment, as was previously determined by EPRI in the EPRI utilization report, the following final 95/95 determination based on the combination of code-specific biases and uncertainties is determined:\n\n\nShow code\n\nEPRIadj <- \n    EPRIuncDataAdj %>%\n    mutate(RSS=round(sqrt(HtoC^2+fTemp^2+regUnc^2))) %>%\n    mutate(utilBias=150+(643-576)) %>%\n    mutate(BandU=round(RSS+utilBias))\nknitr::kable(EPRIadj)\n\n\nburnup\nHtoC\nfTemp\nregUncLow\nregUncHigh\nregUnc\nRSS\nutilBias\nBandU\n10\n380\n255\n250\n333\n366\n586\n217\n803\n20\n452\n255\n500\n667\n733\n898\n217\n1115\n30\n446\n255\n750\n1000\n1098\n1212\n217\n1429\n40\n430\n255\n800\n1167\n1282\n1376\n217\n1593\n50\n410\n255\n900\n1300\n1428\n1507\n217\n1724\n60\n398\n255\n1000\n1400\n1538\n1609\n217\n1826\n\nThe RSS column is the RSS of the uncertainty data, the utilBias is the bias from the EPRI utilization report, and BandU is the sum of the RSS and utilBias columns. The BandU column corresponds to the EPRIstat labels in the report figures.\n\n\n\nANS. 2015. “ANSI/ANS-8.27-2015, Burnup Credit for LWR Fuel.” American Nuclear Society.\n\n\nCummings, Kristopher. 2016. “Response to Request for Additional Information (RAI) Questions Regarding EPRI Report 1025203, ’Utilization of the EPRI Depletion Benchmarks for Burnup Credit Validation,’ and EPRI Report 1022909, ’Benchmarks for Quantifying Fuel Reactivity Depletion Uncertainty’.” Letter. http://www.nrc.gov/docs/ML1610/ML16104A332.html.\n\n\nGauld, IC. 2003. “Strategies for Application of Isotopic Uncertainties in Burnup Credit.” ORNL/TM-2001/257, Oak Ridge National Laboratory. http://wp.ornl.gov/ webworks/cppr/y2003/rpt/112517.pdf.\n\n\nLancaster, Dale. 2012. “Utilization of the EPRI Depletion Benchmarks for Burnup Credit Validation.” Report. Electric Power Research Institute, Palo Alto, CA. http://www.epri.com/abstracts/Pages/ProductAbstract.aspx?ProductId=000000000001025203.\n\n\nMyhre, Janet, Daniel R Jeske, Michael Rennie, and Yingtao Bi. 2010. “Tolerance Intervals in a Heteroscedastic Linear Regression Context with Applications to Aerospace Equipment Surveillance.” International Journal of Quality, Statistics, and Reliability 2009. https://doi.org/10.1155/2009/126283.\n\n\nNatrella, Mary. 2013. NIST/SEMATECH e-Handbook of Statistical Methods. NIST/SEMATECH. http://www.itl.nist.gov/div898/handbook/prc/section2/prc263.htm.\n\n\nORNL. 2011. “SCALE: A Comprehensive Modeling and Simulation Suite for Nuclear Safety Analysis and Design.” Computer Program. ORNL.\n\n\nR Core Team. 2015. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRadulescu, G, I. C. Gauld, G. Ilas, and J. C. Wagner. 2012. “An Approach for Validating Actinide and Fission Product Burnup Credit Criticality Safety Analyses - Isotopic Composition Predictions.” Report. ORNL. http://www.nrc.gov/docs/ML1211/ML12116A124.pdf.\n\n\nShalizi, Cosma Rohilla. 2016. “Advanced Data Analysis from an Elementary Point of View.” http://www.stat.cmu.edu/ cshalizi/ADAfaEPoV.\n\n\nSmith, K., S. Tarves, T. Bahadir, and R. Ferrer. 2011. “Benchmarks for Quantifying Fuel Reactivity Depletion Uncertainty.” Report. Electric Power Research Institute, Palo Alto, CA. http://www.epri.com/abstracts/Pages/ProductAbstract.aspx?ProductId=000000000001022909.\n\n\nWallis, W. Allen. 1951. “Tolerance Intervals for Linear Regression.” http://digitalassets.lib.berkeley.edu/math/ucb/text/math_s2_article-04.pdf.\n\n\nWikipedia. 2016. “Tolerance Interval — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Tolerance_interval&oldid=720051009.\n\n\nYoung, Derek S. 2010. “tolerance: An R Package for Estimating Tolerance Intervals.” Journal of Statistical Software 36 (5): 1–39. http://www.jstatsoft.org/v36/i05/.\n\n\nThese nuclides represent the overwhelming majority of fuel reactivity independent of burnup.↩︎\nThe NEWT 2-D deterministic neutron flux solver was used for transport calculations.↩︎\nSee data error bars in NUREG/CR-7108 Figure 6.1, “Measured-to-calculated concentration ratio versus fuel sample burnup for (a) U-235; (b) Pu-239. The error bars in the graph represent the reported 1-sigma measurement errors; very small measurement errors (0.1%) are not visible on the graph.”↩︎\nThis is likely also the case with the noisy NUREG/CR-7108 measurement data. However, no attempt was made to distinguish measurement uncertainty from true depletion uncertainty (i.e. the noise from the signal). One possible technique is the removal of data with measurement uncertainties known to be prohibitively large, or suspected with high confidence to be, prohibitively large. However, this introduces a qualitative aspect to the analysis and extreme caution would need to be used.↩︎\nIn this analysis, TRITON calls the CSAS5 sequence, which uses KENO-V.a as the three-dimensional stochastic neutron flux solver for transport calculations.↩︎\nThe reason two-sided factors are used in this analysis is because both the direct difference and EPRI dataset prediction intervals were calculated as two-sided intervals. The two-sided tolerance factors were determined using the K.factor function in the R tolerance package.↩︎\nSee discussion “tolerance: An R Package for Estimating Tolerance Intervals,” Journal of Statistical Software, August 2010, Volume 36, Issue 5, Section 5.1, “Linear regression tolerance intervals.”↩︎\nSee entire discussion in Chapter 7, “Moving Beyond Conditional Expectations: Weighted Least Squares, Heteroskedasticity, Local Polynomial Regression,” for more on the reasoning behind choosing a WLS model and the various assumptions used in this analysis.↩︎\nThe Kopp 5% tolerance interval shown is based on a ordinary linear least-squares (OLS) regression model rather than a WLS model. This assumes equal variance weights as a function of burnup (i.e that the data is homoskedastic).↩︎\nSee Section 4, “Criteria to establish subcriticality.”↩︎\nIt is important to keep in mind that the 95/95 uncertainty derived from the tolerance interval corresponding to the Kopp 5% approach in this analysis is not being used in this comparison.↩︎\nSource: http://www.itl.nist.gov/div898/handbook/prc/section2/prc263.r; Discussion: http://www.itl.nist.gov/div898/handbook/prc/section2/prc263.htm↩︎\nFrom “tolerance: An R Package for Estimating Tolerance Intervals,” Journal of Statistical Software, August 2010, Volume 36, Issue 5 Article available at: https://www.jstatsoft.org/article/view/v036i05/v36i05.pdf↩︎\n",
    "preview": "papers/2016-06-09-depletion-uncertainty-meta-analysis/depletion-uncertainty-meta-analysis_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-02-01T13:56:10+05:45",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "papers/2015-05-15-case-studies-examining-spent-fuel-pool-criticality-uncertainty-analysis/",
    "title": "Case Studies Examining Spent Fuel Pool Criticality Uncertainty Analysis",
    "description": "Commercial nuclear power plant (NPP) operating strategies, spent fuel pool (SFP) management practices, and nuclear fuel design have evolved to a point where it is common for SFP nuclear criticality safety (NCS) analyses to show little remaining margin to regulatory limits for subcriticality. As a result, ensuring that all significant sources of uncertainty or bias are properly considered and accounted for in the NCS analyses has taken on increased significance. A large amount of SFP NCS analysis work has been performed in recent years that includes investigation of a broad variety of SFP storage configurations and possible reactivity sensitivities that could result in erosion of existing subcriticality margins. Due to ongoing efforts to develop detailed guidance for performing SFP NCS analyses that will be used by commercial NPP licensees, NRC staff has been performing a review of documents submitted to the NRC with uncertainty and sensitivity related information, including approved license amendment requests and publicly available studies. This paper presents some of the resulting findings and recommendations for future study.",
    "author": [
      {
        "name": "Amrit Patel",
        "url": {}
      },
      {
        "name": "Scott T. Krepel",
        "url": {}
      }
    ],
    "date": "2015-05-15",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nDatabase Overview\nCase Studies\nFuel Rod Pitch Manufacturing Tolerance Reactivity Effect\nFuel Asmembly Eccentric Position Reactivity Effect\nUO2 Fuel Density Uncertainty\n\nConclusions\nAcknowledgements\n\n\n\n\nIntroduction\nIn order to demonstrate that they meet the NRC’s subcriticality requirements, licensees must submit analyses that demonstrate that the limit on SFP k-effective is satisfied at the upper 95/95 statistical threshold. Historically, many licensees submitted simple analyses that either treated parameters affecting SFP criticality in a bounding manner, or had significant margin to the limit to cover any unevaluated uncertainties. In recent years, many power plant operators have been submitting SFP NCS analyses that show little remaining margin to the regulatory limit. This is generally because of a number of recent evolutions in SFP and plant operations, such as re-racking for higher density SFP storage, increased cycle energy requirements that result in storage of higher reactivity fuel, or degradation of neutron-absorbing materials (NAM). Therefore, the individual contributors to the bias and uncertainty are an important review area for the NRC staff. In order to assess how the different uncertainties/biases behave under a variety of real world applications, the NRC staff has developed a database containing information from license amendment request (LAR) documentation. The insights obtained through analysis of this information will inform NRC staff’s evaluation of any uncertainty/bias related recommendations proposed as part of the new SFP criticality guidance in NEI 12-16, “Guidance for Performing Criticality Analyses of Fuel Storage at Light-Water Reactor Power Plants.”1\nDatabase Overview\nAs part of ongoing efforts to determine the expected reactivity impact due to specific uncertainties/biases, NRC staff has been performing a review of approved LARs which include information on uncertainties and biases in SFP NCS analyses. This document review has allowed the NRC to consolidate and assess a large amount of SFP NCS analysis data from NRC approved real world applications. This data is relevant because (1) the SFP NCS data was produced through methodologies that have been reviewed and approved by the NRC, and (2) it represents practical application of NCS analysis methodologies, rather than theoretical situations established for the purposes of sensitivity studies. These two characteristics make this data set unique in that they address the main reasons which may limit the applicability of any conclusions developed as a result of data analysis.\nThe data was collected from 33 approved SFP LARs dating back to the year 2000, which resulted in a database consisting of results from 163 individual criticality calculations. LARs that were withdrawn or LARs where NRC staff did not explicitly review the NCS analysis were not included in the database. Additionally, LARs older than ten years are not considered to be as reliable as more recent LARs because they generally did not challenge regulatory limits. Furthermore, for the older LARs, not every single uncertainty or bias term was included in the documentation supporting the LAR. However, the more common uncertainties and biases are tracked, as well as any large uncertainties that may be specific to certain methodologies. Information related to details of each SFP configuration was also collected, such as fuel design type, rack type (i.e., flux trap or egg crate), NAM including boron-10 areal density, etc. This information may be useful in determining whether specific conclusions may only apply to specific storage configurations or characteristics.\nThe NRC currently has plans to leverage the SFP NCS uncertainty database in several ways. Some examples include: (1) establishing the significance of specific uncertainties (e.g. those associated with manufacturing tolerances), (2) investigating the variability of specific uncertainties, (3) looking for correlations or trends between uncertainties and various parameters (e.g., rack type or NAM areal density), and (4) identifying outliers that challenge conventional wisdom. Any findings from analysis of the data in this database will be used to inform NRC staff reviews of new LARs and NCS analysis guidance.\nCase Studies\nThe SFP NCS database currently contains a disproportionate amount of PWR data. Therefore, three pressurized water reactor (PWR) SFP case studies are presented: (1) variation in fuel rod pitch manufacturing tolerance uncertainty, (2) observed outliers associated with the fuel assembly eccentric position reactivity effect, and (3) variation in uranium-dioxide (UO2) fuel density uncertainty. Due to the nature of analysis methodologies employed in the past to support boron credit for PWR SFPs, the available data for borated conditions is relatively limited. Therefore, the case studies documented here focus on unborated conditions in PWR SFPs.\nFuel Rod Pitch Manufacturing Tolerance Reactivity Effect\nA comparison of the reactivity effects due to varying fuel rod pitch for different fuel design types in the SFP NCS database is shown in Figure 1. Fuel rod pitch uncertainty data exists in the database for 11 PWR fuel design types. In this paper, “fuel design type” is used to refer to a fuel assembly design marketed by a fuel vendor that has a unique geometry and non-fuel material composition. The fuel design types are labeled on the x-axis of Figure 1 with alphabetic designations to protect proprietary information.\n\n\n\nFigure 1: Fuel rod pitch uncertainty variation with fuel design type.\n\n\n\nFigure 1 shows that fuel design type A displays a significantly higher reactivity increase than the other fuel design types due to fuel rod pitch tolerances. Fuel design types A and B have a similar number of fuel pins, so the observed variation is not specific to the rod array size. The data associated with fuel design types A and B was filtered to identify pairs where the fuel design type was different, but a number of the characteristics of the fuel assembly and SFP rack cell were very similar (i.e., same rod array size, similar enrichment, similar burnup, similar SFP rack cell configuration, lack of NAM, and same SFP fuel loading pattern). Several such pairs were identified, so it became clear that there can be drastic variation in the reactivity impact of the fuel rod pitch, even when many of the parameters used to characterize the geometry and composition are very similar. It was not clear whether this difference is attributable mainly to the difference in fuel design type or NCS analysis methodology.\nIn the criticality calculation results investigated, one parameter that could cause significant difference is the relative percent change in fuel rod pitch. If the change in fuel rod pitch is significantly larger for the data associated with fuel design type A due to larger manufacturing tolerances, this could explain the unexpectedly large difference. For one of the NCS analysis cases for fuel design type A, the limiting rod pitch change was a decrease of 1.35%, which resulted in an uncertainty of about 1.5% delta-k-effective (dk). For one of the fuel design type B data points with similar fuel assembly and SFP rack cell characteristics, the limiting rod pitch change was an increase of 2.60%, resulting in an uncertainty of about 0.25% dk. The results showed that for one case, there was a large reactivity effect for a smaller change relative to the other case, and the direction of variation in fuel rod pitch is different for the two cases. For the fuel design type A case, the pitch was perturbed so that the lattice became tighter and closer together, while the perturbation in the fuel design type B lattice resulted in the fuel rods being spread more apart.\nFigure 2 displays data from all fuel design types, and shows that an increase in reactivity may be associated with either an increase or decrease in fuel rod pitch for different analyzed configurations. Figure 2 also demonstrates how different contributors to the variance of a potential correlation may be considered. In the first plot, the NAM areal density doesn’t seem to have a clear impact on the sensitivity of the rack reactivity to fuel rod pitch changes, suggesting that the rod pitch uncertainty is independent of the amount of NAM in the racks. The second plot shows that flux trap racks tend to show a higher sensitivity to changes in fuel rod pitch compared to egg crate racks. This implies that the rack design shows potential as a weak contributor to the rod pitch reactivity. The sample sizes are such that definitive conclusions can’t be drawn, but this is a potential avenue for future study or data collection. The variation in fuel design types H and I also indicate further study is needed to filter out some of the noise and identify other potential variance contributors.\n\n\n\n\n\n\nFigure 2: Fuel rod pitch uncertainty variation with fuel rod pitch change.\n\n\n\nThe fuel rod pitch uncertainty appears to be more sensitive to negative fuel rod pitch changes than positive changes, but this cannot be confirmed based solely on this analysis. The limiting direction for fuel rod pitch perturbation will most likely be dependent on the particular fuel design, or more specifically, the fuel-to-moderator ratio. Furthermore, at the negative fuel rod pitch change extreme as calculated for fuel design type A, all of the data points come from the same LAR. This makes it difficult to say if the fuel design type A data are outliers (due to methodology or physics) or if the fuel design type A data are consistent with the trend implied by the rest of the data. Regardless of the correct physical explanation for the behavior of the reactivity associated with fuel rod pitch uncertainty perturbations, this case study affirms the importance of performing manufacturing tolerance perturbations in both positive and negative directions in order to determine the limiting effect.\nFuel Asmembly Eccentric Position Reactivity Effect\n\n\n\nAnalysis of the reactivity effect due to fuel assembly location variability within a storage rack cell, or eccentric position, has been analyzed for 125 cases where data is available. Figure 3 shows the broad range of variability in the reactivity effect associated with fuel assembly eccentric position.\n\n\n\nFigure 3: Distribution of eccentric position reactivity effect.\n\n\n\n\n\n\nFigure 4: Eccentric position reactivity effect variation with NAM areal density.\n\n\n\nThe highest reactivity effect due to eccentric positioning of fuel within the SFP cells comes from a storage configuration that utilized storage rack inserts in a repeating one-of-four pattern in the storage cells. Such geometry, intuitively, would be expected to result in potentially large changes in reactivity when the NAM is a significant contributor to subcriticality and adjacent fuel assemblies are pushed closer together in areas that are not shielded by the NAM. The remaining eccentric position reactivity effects that seem to be particularly high are from analyses involving flux trap racks without NAM. Figure 4 shows that the calculated eccentric positioning reactivity effect tends to increase as the boron-10 areal density of the NAM installed in the rack approaches zero. Figure 4 also shows that flux trap designs tend to produce larger eccentric position reactivity effects compared to egg crate rack designs at zero boron-10 areal density. There is significant scatter in this data because there are significant contributing factors from the SFP rack configuration and SFP fuel loading pattern used in specific criticality analyses. As pointed out in recent EPRI-sponsored sensitivity studies, the cross-sectional area of the fuel assembly in relation to the storage cell cross-sectional area could cause significant variation in eccentric position (i.e. some fuel will fit tighter in storage cells than others, meaning less room to be eccentrically loaded).2 Consequently, pinpointing the exact cause of the higher eccentric position reactivity effect can be difficult.\nUO2 Fuel Density Uncertainty\n\n\n\nAnalysis of UO2 fuel density uncertainty has been analyzed for 80 cases where data is available. Initial inspection of the data revealed that the reactivity effect from increases in UO2 fuel density due to manufacturing tolerances (i.e., the fuel density uncertainty) tends to increase with increasing fuel burnup. This trend is apparent in Figure 5. The two data points that deviate the most from this trend are part of the same SFP NCS analysis. There is no obvious cause for these significant deviations from the typical fuel density uncertainty distribution.\n\n\n\nFigure 5: Fuel density uncertainty variation with fuel enrichment.\n\n\n\nAlthough it does appear that there may be an increase in fuel density uncertainty with burnup, Figure 5 shows several points that do not follow this trend. By comparing points corresponding to fresh fuel analyses with those corresponding to spent fuel analyses, fuel density uncertainties from spent fuel analyses at high burnups were identified that are similar to fresh fuel analysis results. The NRC staff is aware of applications where the fuel density uncertainty was determined at a lower burnup, and then the same fuel density uncertainty was used at higher burnups without performing calculations to support that extrapolation. That may be the main cause of the strong similarity between some of the fuel density uncertainties calculated for low and high burnups. This case study clarifies that fuel density uncertainty trends as a function of burnup should at least be considered, so calculating this uncertainty once at the fresh fuel condition and assuming it is valid for all burnup/enrichment cases may not be appropriate.\nConclusions\nThe case studies presented confirm that determination of the uncertainties associated with SFP NCS criticality analyses requires careful consideration of potential uncertainty dependencies.\nThree specific causes of reactivity uncertainties/biases were examined: the fuel rod pitch manufacturing tolerance, eccentric fuel position, and the fuel density manufacturing tolerance. In the first case, the fuel rod pitch uncertainty sensitivity studies demonstrate that fuel rod pitch changes in both directions should be considered to identify the limiting uncertainty for use in safety analyses. Secondly, the NAM condition was determined to have a potentially significant impact on the reactivity associated with eccentric fuel position in SFP storage cells, particularly for degraded or low areal density NAMs and nonuniformly oriented NAMs. Finally, potential burnup dependence was identified for the uncertainty associated with the fuel density manufacturing tolerance, indicating that it is not appropriate to assume that fuel density uncertainties calculated for fresh fuel will be applicable to burned fuel.\nThe NRC is continuing to augment and analyze the SFP NCS database in order to establish the significance of specific uncertainties, investigate the variability of those uncertainties, look for trends between uncertainties and various parameters, and identify/understand potential outliers. These efforts will help identify or confirm important considerations when performing SFP NCS uncertainty analysis, and inform ongoing SFP NCS guidance development.\nAcknowledgements\nThis research was performed with the support of Kent Wood and Chris Jackson, members of NRC’s Reactor Systems branch in the Office of Nuclear Reactor Regulation, Division of Safety Systems.\n\n“Guidance for Performing Criticality Analyses of Fuel Storage at Light-Water Reactor Power Plants,” http://pbadupws.nrc.gov/docs/ML1411/ML14112A516.pdf (2014).↩︎\n“Sensitivity Analyses for Spent Fuel Pool Criticality,” http://www.epri.com/abstracts/Pages/ProductAbstract.aspx?ProductId=000000003002003073 (2014).↩︎\n",
    "preview": "papers/2015-05-15-case-studies-examining-spent-fuel-pool-criticality-uncertainty-analysis/sfp-criticality-uncertainty-analysis-case-studies_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-01T14:37:48+05:45",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
